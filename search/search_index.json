{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AutoAWQ","text":"<p>AutoAWQ pushes ease of use and fast inference speed into one package. In the following documentation, you will learn how to quantize and run inference.</p> <p>Example inference speed (RTX 4090, Ryzen 9 7950X, 64 tokens):</p> <ul> <li>Vicuna 7B (GEMV kernel): 198.848 tokens/s</li> <li>Mistral 7B (GEMM kernel): 156.317 tokens/s</li> <li>Mistral 7B (ExLlamaV2 kernel): 188.865 tokens/s</li> <li>Mixtral 46.7B (GEMM kernel): 93 tokens/s (2x 4090)</li> </ul>"},{"location":"#installation-notes","title":"Installation notes","text":"<ul> <li>Install: <code>pip install autoawq</code>.</li> <li>Your torch version must match the build version, i.e. you cannot use torch 2.0.1 with a wheel that was built with 2.2.0.</li> <li>For AMD GPUs, inference will run through ExLlamaV2 kernels without fused layers. You need to pass the following arguments to run with AMD GPUs:     <pre><code>model = AutoAWQForCausalLM.from_quantized(\n    ...,\n    fuse_layers=False,\n    use_exllama_v2=True\n)\n</code></pre></li> <li>For CPU device, you should install intel_extension_for_pytorch with <code>pip install intel_extension_for_pytorch</code>. And the latest version of torch is required since \"intel_extension_for_pytorch(IPEX)\" was built with the latest version of torch(now IPEX 2.4 was build with torch 2.4). If you build IPEX from source code, then you need to ensure the consistency of the torch version. And you should use \"use_ipex=True\" for CPU device.     <pre><code>model = AutoAWQForCausalLM.from_quantized(\n    ...,\n    use_ipex=True\n)\n</code></pre></li> </ul>"},{"location":"#supported-models","title":"Supported models","text":"<p>We support modern LLMs. You can find a list of supported Huggingface <code>model_types</code> in <code>awq/models</code>.</p>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#basic-quantization","title":"Basic Quantization","text":"<p>AWQ performs zero point quantization down to a precision of 4-bit integers. You can also specify other bit rates like 3-bit, but some of these options may lack kernels for running inference.</p> <p>Notes:</p> <ul> <li>Some models like Falcon is only compatible with group size 64.</li> <li>To use Marlin, you must specify zero point as False and version as Marlin.</li> </ul> <pre><code>from awq import AutoAWQForCausalLM\nfrom transformers import AutoTokenizer\n\nmodel_path = 'mistralai/Mistral-7B-Instruct-v0.2'\nquant_path = 'mistral-instruct-v0.2-awq'\nquant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\n\n# Load model\nmodel = AutoAWQForCausalLM.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n\n# Quantize\nmodel.quantize(tokenizer, quant_config=quant_config)\n\n# Save quantized model\nmodel.save_quantized(quant_path)\ntokenizer.save_pretrained(quant_path)\n\nprint(f'Model is quantized and saved at \"{quant_path}\"')\n</code></pre>"},{"location":"examples/#custom-data","title":"Custom Data","text":"<p>This includes an example function that loads either wikitext or dolly. Note that currently all samples above 512 in length are discarded.</p> <pre><code>from datasets import load_dataset\nfrom awq import AutoAWQForCausalLM\nfrom transformers import AutoTokenizer\n\nmodel_path = 'lmsys/vicuna-7b-v1.5'\nquant_path = 'vicuna-7b-v1.5-awq'\nquant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\n\n# Load model\nmodel = AutoAWQForCausalLM.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n\n# Define data loading methods\ndef load_dolly():\n    data = load_dataset('databricks/databricks-dolly-15k', split=\"train\")\n\n    # concatenate data\n    def concatenate_data(x):\n        return {\"text\": x['instruction'] + '\\n' + x['context'] + '\\n' + x['response']}\n\n    concatenated = data.map(concatenate_data)\n    return [text for text in concatenated[\"text\"]]\n\ndef load_wikitext():\n    data = load_dataset('wikitext', 'wikitext-2-raw-v1', split=\"train\")\n    return [text for text in data[\"text\"] if text.strip() != '' and len(text.split(' ')) &gt; 20]\n\n# Quantize\nmodel.quantize(tokenizer, quant_config=quant_config, calib_data=load_wikitext())\n\n# Save quantized model\nmodel.save_quantized(quant_path)\ntokenizer.save_pretrained(quant_path)\n\nprint(f'Model is quantized and saved at \"{quant_path}\"')\n</code></pre>"},{"location":"examples/#long-context-optimizing-quantization","title":"Long-context: Optimizing quantization","text":"<p>For this example, we will use HuggingFaceTB/cosmopedia-100k as it's a high-quality dataset and we can filter directly on the number of tokens. We will use Qwen2 7B, one of the newer supported models in AutoAWQ which is high-performing. The following example ran smoothly on a machine with an RTX 4090 24 GB VRAM with 107 GB system RAM.</p> <p>NOTE: Adjusting <code>n_parallel_calib_samples</code>, <code>max_calib_samples</code>, and <code>max_calib_seq_len</code> will help avoid OOM when customizing your dataset.</p> <ul> <li>The AWQ algorithm is incredibly sample efficient, so <code>max_calib_samples</code> of 128-256 should be sufficient to quantize a model. A higher number of samples may not be possible without significant memory available or without further optimizing AWQ with a PR for disk offload.</li> <li>When <code>n_parallel_calib_samples</code> is set to an integer, we offload to system RAM to save GPU VRAM. This may cause OOM on your system if you have little memory available; we are looking to optimize this further in future versions.</li> </ul> <pre><code>from datasets import load_dataset\nfrom awq import AutoAWQForCausalLM\nfrom transformers import AutoTokenizer\n\nmodel_path = 'Qwen/Qwen2-7B-Instruct'\nquant_path = 'qwen2-7b-awq'\nquant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\n\n# Load model\nmodel = AutoAWQForCausalLM.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n\ndef load_cosmopedia():\n    data = load_dataset('HuggingFaceTB/cosmopedia-100k', split=\"train\")\n    data = data.filter(lambda x: x[\"text_token_length\"] &gt;= 2048)\n\n    return [text for text in data[\"text\"]]\n\n# Quantize\nmodel.quantize(\n    tokenizer,\n    quant_config=quant_config,\n    calib_data=load_cosmopedia(),\n    n_parallel_calib_samples=32,\n    max_calib_samples=128,\n    max_calib_seq_len=4096\n)\n\n# Save quantized model\nmodel.save_quantized(quant_path)\ntokenizer.save_pretrained(quant_path)\n\nprint(f'Model is quantized and saved at \"{quant_path}\"')\n</code></pre>"},{"location":"examples/#coding-models","title":"Coding models","text":"<p>For this example, we will use deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct as it's an excellent coding model.</p> <pre><code>from tqdm import tqdm\nfrom datasets import load_dataset\nfrom awq import AutoAWQForCausalLM\nfrom transformers import AutoTokenizer\n\nmodel_path = 'deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct'\nquant_path = 'deepseek-coder-v2-lite-instruct-awq'\nquant_config = { \"zero_point\": True, \"q_group_size\": 64, \"w_bit\": 4, \"version\": \"GEMM\" }\n\n# Load model\nmodel = AutoAWQForCausalLM.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n\ndef load_openhermes_coding():\n    data = load_dataset(\"alvarobartt/openhermes-preferences-coding\", split=\"train\")\n\n    samples = []\n    for sample in data:\n        responses = [f'{response[\"role\"]}: {response[\"content\"]}' for response in sample[\"chosen\"]]\n        samples.append(\"\\n\".join(responses))\n\n    return samples\n\n# Quantize\nmodel.quantize(\n    tokenizer,\n    quant_config=quant_config,\n    calib_data=load_openhermes_coding(),\n    # MODIFY these parameters if need be:\n    # n_parallel_calib_samples=32,\n    # max_calib_samples=128,\n    # max_calib_seq_len=4096\n)\n\n# Save quantized model\nmodel.save_quantized(quant_path)\ntokenizer.save_pretrained(quant_path)\n\nprint(f'Model is quantized and saved at \"{quant_path}\"')\n</code></pre>"},{"location":"examples/#vision-language-models","title":"Vision-Language Models","text":"<p>AutoAWQ supports a few vision-language models. So far, we support LLaVa 1.5 and LLaVa 1.6 (next).</p> <pre><code>from awq import AutoAWQForCausalLM\nfrom transformers import AutoTokenizer\n\nmodel_path = 'llava-hf/llama3-llava-next-8b-hf'\nquant_path = 'llama3-llava-next-8b-awq'\nquant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\n\n# Load model\nmodel = AutoAWQForCausalLM.from_pretrained(\n    model_path, low_cpu_mem_usage=True,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n\n# Quantize\nmodel.quantize(tokenizer, quant_config=quant_config)\n\n# Save quantized model\nmodel.save_quantized(quant_path)\ntokenizer.save_pretrained(quant_path)\n\nprint(f'Model is quantized and saved at \"{quant_path}\"')\n</code></pre>"},{"location":"examples/#gguf-export","title":"GGUF Export","text":"<p>This computes AWQ scales and appliesthem to the model without running real quantization. This keeps the quality of AWQ because theweights are applied but skips quantization in order to make it compatible with other frameworks.</p> <p>Step by step:</p> <ul> <li><code>quantize()</code>: Compute AWQ scales and apply them</li> <li><code>save_pretrained()</code>: Saves a non-quantized model in FP16</li> <li><code>convert.py</code>: Convert the Huggingface FP16 weights to GGUF FP16 weights</li> <li><code>quantize</code>: Run GGUF quantization to get real quantized weights, in this case 4-bit.</li> </ul> <pre><code>import os\nimport subprocess\nfrom awq import AutoAWQForCausalLM\nfrom transformers import AutoTokenizer\n\nmodel_path = 'mistralai/Mistral-7B-v0.1'\nquant_path = 'mistral-awq'\nllama_cpp_path = '/workspace/llama.cpp'\nquant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 6, \"version\": \"GEMM\" }\n\n# Load model\nmodel = AutoAWQForCausalLM.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n\n# Quantize\n# NOTE: We avoid packing weights, so you cannot use this model in AutoAWQ\n# after quantizing. The saved model is FP16 but has the AWQ scales applied.\nmodel.quantize(\n    tokenizer,\n    quant_config=quant_config,\n    export_compatible=True\n)\n\n# Save quantized model\nmodel.save_quantized(quant_path)\ntokenizer.save_pretrained(quant_path)\nprint(f'Model is quantized and saved at \"{quant_path}\"')\n\n# GGUF conversion\nprint('Converting model to GGUF...')\nllama_cpp_method = \"q4_K_M\"\nconvert_cmd_path = os.path.join(llama_cpp_path, \"convert.py\")\nquantize_cmd_path = os.path.join(llama_cpp_path, \"quantize\")\n\nif not os.path.exists(llama_cpp_path):\n    cmd = f\"git clone https://github.com/ggerganov/llama.cpp.git {llama_cpp_path} &amp;&amp; cd {llama_cpp_path} &amp;&amp; make LLAMA_CUBLAS=1 LLAMA_CUDA_F16=1\"\n    subprocess.run([cmd], shell=True, check=True)\n\nsubprocess.run([\n    f\"python {convert_cmd_path} {quant_path} --outfile {quant_path}/model.gguf\"\n], shell=True, check=True)\n\nsubprocess.run([\n    f\"{quantize_cmd_path} {quant_path}/model.gguf {quant_path}/model_{llama_cpp_method}.gguf {llama_cpp_method}\"\n], shell=True, check=True)\n</code></pre>"},{"location":"examples/#custom-quantizer-qwen2-vl-example","title":"Custom Quantizer (Qwen2 VL Example)","text":"<p>Below, the Qwen team has provided an example of how to use a custom quantizer. This works to effectively quantize the Qwen2 VL model using multimodal examples.</p> <pre><code>import torch\nimport torch.nn as nn\n\nfrom awq import AutoAWQForCausalLM\nfrom awq.utils.qwen_vl_utils import process_vision_info\nfrom awq.quantize.quantizer import AwqQuantizer, clear_memory, get_best_device\n\n# Specify paths and hyperparameters for quantization\nmodel_path = \"Qwen/Qwen2-VL-7B-Instruct\"\nquant_path = \"qwen2-vl-7b-instruct\"\nquant_config = {\"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\"}\n\nmodel = AutoAWQForCausalLM.from_pretrained(\n    model_path, attn_implementation=\"flash_attention_2\"\n)\n\n# We define our own quantizer by extending the AwqQuantizer.\n# The main difference is in how the samples are processed when\n# the quantization process initialized.\nclass Qwen2VLAwqQuantizer(AwqQuantizer):\n    def init_quant(self, n_samples=None, max_seq_len=None):\n        modules = self.awq_model.get_model_layers(self.model)\n        samples = self.calib_data\n\n        inps = []\n        layer_kwargs = {}\n\n        best_device = get_best_device()\n        modules[0] = modules[0].to(best_device)\n        self.awq_model.move_embed(self.model, best_device)\n\n        # get input and kwargs to layer 0\n        # with_kwargs is only supported in PyTorch 2.0\n        # use this Catcher hack for now\n        class Catcher(nn.Module):\n            def __init__(self, module):\n                super().__init__()\n                self.module = module\n\n            def forward(self, *args, **kwargs):\n                # assume first input to forward is hidden states\n                if len(args) &gt; 0:\n                    hidden_states = args[0]\n                    del args\n                else:\n                    first_key = list(kwargs.keys())[0]\n                    hidden_states = kwargs.pop(first_key)\n\n                inps.append(hidden_states)\n                layer_kwargs.update(kwargs)\n                raise ValueError  # early exit to break later inference\n\n        def move_to_device(obj: torch.Tensor | nn.Module, device: torch.device):\n            def get_device(obj: torch.Tensor | nn.Module):\n                if isinstance(obj, torch.Tensor):\n                    return obj.device\n                return next(obj.parameters()).device\n\n            if get_device(obj) != device:\n                obj = obj.to(device)\n            return obj\n\n        # patch layer 0 to catch input and kwargs\n        modules[0] = Catcher(modules[0])\n        for k, v in samples.items():\n            if isinstance(v, (torch.Tensor, nn.Module)):\n                samples[k] = move_to_device(v, best_device)\n        try:\n            self.model(**samples)\n        except ValueError:  # work with early exit\n            pass\n        finally:\n            for k, v in samples.items():\n                if isinstance(v, (torch.Tensor, nn.Module)):\n                    samples[k] = move_to_device(v, \"cpu\")\n        modules[0] = modules[0].module  # restore\n\n        del samples\n        inps = inps[0]\n\n        modules[0] = modules[0].cpu()\n        self.awq_model.move_embed(self.model, \"cpu\")\n\n        clear_memory()\n\n        return modules, layer_kwargs, inps\n\n# Then you need to prepare your data for calibaration. What you need to do is just put samples into a list,\n# each of which is a typical chat message as shown below. you can specify text and image in `content` field:\n# dataset = [\n#     # message 0\n#     [\n#         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n#         {\"role\": \"user\", \"content\": \"Tell me who you are.\"},\n#         {\"role\": \"assistant\", \"content\": \"I am a large language model named Qwen...\"},\n#     ],\n#     # message 1\n#     [\n#         {\n#             \"role\": \"user\",\n#             \"content\": [\n#                 {\"type\": \"image\", \"image\": \"file:///path/to/your/image.jpg\"},\n#                 {\"type\": \"text\", \"text\": \"Output all text in the image\"},\n#             ],\n#         },\n#         {\"role\": \"assistant\", \"content\": \"The text in the image is balabala...\"},\n#     ],\n#     # other messages...\n#     ...,\n# ]\n# here, we use a caption dataset **only for demonstration**. You should replace it with your own sft dataset.\ndef prepare_dataset(n_sample: int = 8) -&gt; list[list[dict]]:\n    from datasets import load_dataset\n\n    dataset = load_dataset(\"laion/220k-GPT4Vision-captions-from-LIVIS\", split=f\"train[:{n_sample}]\")\n    return [\n        [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"image\", \"image\": sample[\"url\"]},\n                    {\"type\": \"text\", \"text\": \"generate a caption for this image\"},\n                ],\n            },\n            {\"role\": \"assistant\", \"content\": sample[\"caption\"]},\n        ]\n        for sample in dataset\n    ]\n\ndataset = prepare_dataset()\n\n# process the dataset into tensors\ntext = model.processor.apply_chat_template(dataset, tokenize=False, add_generation_prompt=True)\nimage_inputs, video_inputs = process_vision_info(dataset)\ninputs = model.processor(text=text, images=image_inputs, videos=video_inputs, padding=True, return_tensors=\"pt\")\n\n# Then just run the calibration process by one line of code\nmodel.quantize(calib_data=inputs, quant_config=quant_config, quantizer_cls=Qwen2VLAwqQuantizer)\n\n# Save the model\nmodel.model.config.use_cache = model.model.generation_config.use_cache = True\nmodel.save_quantized(quant_path, safetensors=True, shard_size=\"4GB\")\n</code></pre>"},{"location":"examples/#another-custom-quantizer-minicpm3-example","title":"Another Custom Quantizer (MiniCPM3 Example)","text":"<p>Here we introduce another custom quantizer from the MiniCPM team at OpenBMB. We only modify the weight clipping mechanism to make quantization work.</p> <pre><code>import torch\nfrom transformers import AutoTokenizer\n\nfrom awq import AutoAWQForCausalLM\nfrom awq.quantize.quantizer import AwqQuantizer, clear_memory\n\nclass CPM3AwqQuantizer(AwqQuantizer):\n    @torch.no_grad()\n    def _compute_best_clip(\n        self,\n        w: torch.Tensor,\n        input_feat: torch.Tensor,\n        n_grid=20,\n        max_shrink=0.5,\n        n_sample_token=512,\n    ):\n        assert w.dim() == 2\n        org_w_shape = w.shape\n        # w           [co, ci]      -&gt; [co, 1, n_group, group size]\n        # input_feat  [n_token, ci] -&gt; [1, n_token, n_group, group size]\n        group_size = self.group_size if self.group_size &gt; 0 else org_w_shape[1]\n        input_feat = input_feat.view(-1, input_feat.shape[-1])\n        input_feat = input_feat.reshape(1, input_feat.shape[0], -1, group_size)\n\n        # Compute input feature step size (minimum 1)\n        step_size = max(1, input_feat.shape[1] // n_sample_token)\n        input_feat = input_feat[:, ::step_size]\n\n        w = w.reshape(org_w_shape[0], 1, -1, group_size)\n\n        oc_batch_size = 256 if org_w_shape[0] % 256 == 0 else 64  # prevent OOM\n        if org_w_shape[0] % oc_batch_size != 0:\n            oc_batch_size = org_w_shape[0]\n        assert org_w_shape[0] % oc_batch_size == 0\n        w_all = w\n        best_max_val_all = []\n\n        for i_b in range(org_w_shape[0] // oc_batch_size):\n            w = w_all[i_b * oc_batch_size : (i_b + 1) * oc_batch_size]\n\n            org_max_val = w.abs().amax(dim=-1, keepdim=True)  # co, 1, n_group, 1\n\n            best_max_val = org_max_val.clone()\n            min_errs = torch.ones_like(org_max_val) * 1e9\n            input_feat = input_feat.to(w.device)\n            org_out = (input_feat * w).sum(dim=-1)  # co, n_token, n_group\n\n            for i_s in range(int(max_shrink * n_grid)):\n                max_val = org_max_val * (1 - i_s / n_grid)\n                min_val = -max_val\n                cur_w = torch.clamp(w, min_val, max_val)\n                q_w = self.pseudo_quantize_tensor(cur_w)[0]\n                cur_out = (input_feat * q_w).sum(dim=-1)\n\n                # co, 1, n_group, 1\n                err = (cur_out - org_out).pow(2).mean(dim=1).view(min_errs.shape)\n                del cur_w\n                del cur_out\n                cur_best_idx = err &lt; min_errs\n                min_errs[cur_best_idx] = err[cur_best_idx]\n                best_max_val[cur_best_idx] = max_val[cur_best_idx]\n            best_max_val_all.append(best_max_val)\n\n        best_max_val = torch.cat(best_max_val_all, dim=0)\n\n        clear_memory(input_feat)\n        clear_memory(org_out)\n\n        return best_max_val.squeeze(1)\n\nmodel_path = 'openbmb/MiniCPM3-4B'\nquant_path = 'minicpm3-4b-awq'\nquant_config = { \"zero_point\": True, \"q_group_size\": 64, \"w_bit\": 4, \"version\": \"GEMM\" }\n\n# Load model\nmodel = AutoAWQForCausalLM.from_pretrained(model_path, safetensors=False)\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n\n# Quantize\nmodel.quantize(tokenizer, quant_config=quant_config, quantizer_cls=CPM3AwqQuantizer)\n\n# Save quantized model\nmodel.save_quantized(quant_path)\ntokenizer.save_pretrained(quant_path)\n\nprint(f'Model is quantized and saved at \"{quant_path}\"')\n</code></pre>"},{"location":"examples/#basic-inference","title":"Basic Inference","text":""},{"location":"examples/#inference-with-gpu","title":"Inference With GPU","text":"<p>To run inference, you often want to run with <code>fuse_layers=True</code> to get the claimed speedup in AutoAWQ. Additionally, consider setting <code>max_seq_len</code> (default: 2048) as this will be the maximum context that the model can hold.</p> <p>Notes:</p> <ul> <li>You can specify <code>use_exllama_v2=True</code> to enable ExLlamaV2 kernels during inference.</li> </ul> <pre><code>from awq import AutoAWQForCausalLM\nfrom transformers import AutoTokenizer, TextStreamer\n\nquant_path = \"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\"\n\n# Load model\nmodel = AutoAWQForCausalLM.from_quantized(quant_path, fuse_layers=True)\ntokenizer = AutoTokenizer.from_pretrained(quant_path, trust_remote_code=True)\nstreamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n\n# Convert prompt to tokens\nprompt_template = \"[INST] {prompt} [/INST]\"\n\nprompt = \"You're standing on the surface of the Earth. \"\\\n        \"You walk one mile south, one mile west and one mile north. \"\\\n        \"You end up exactly where you started. Where are you?\"\n\ntokens = tokenizer(\n    prompt_template.format(prompt=prompt), \n    return_tensors='pt'\n).input_ids.cuda()\n\n# Generate output\ngeneration_output = model.generate(\n    tokens, \n    streamer=streamer,\n    max_new_tokens=512\n)\n</code></pre>"},{"location":"examples/#inference-with-cpu","title":"Inference With CPU","text":"<p>To run inference with CPU , you should specify <code>use_ipex=True</code>. ipex is the backend for CPU including kernel for operators. ipex is intel_extension_for_pytorch package.</p> <pre><code>from awq import AutoAWQForCausalLM\n\nquant_path = \"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\"\n# Load model\nmodel = AutoAWQForCausalLM.from_quantized(quant_path, use_ipex=True)\n</code></pre>"},{"location":"examples/#transformers","title":"Transformers","text":"<p>You can also load an AWQ model by using AutoModelForCausalLM, just make sure you have AutoAWQ installed. Note that not all models will have fused modules when loading from transformers. See more documentation here.</p> <pre><code>import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n\n# NOTE: Must install from PR until merged\n# pip install --upgrade git+https://github.com/younesbelkada/transformers.git@add-awq\nmodel_id = \"casperhansen/mistral-7b-instruct-v0.1-awq\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id, \n    torch_dtype=torch.float16, \n    low_cpu_mem_usage=True,\n)\nstreamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n\n# Convert prompt to tokens\ntext = \"[INST] What are the basic steps to use the Huggingface transformers library? [/INST]\"\n\ntokens = tokenizer(\n    text, \n    return_tensors='pt'\n).input_ids.cuda()\n\n# Generate output\ngeneration_output = model.generate(\n    tokens, \n    streamer=streamer,\n    max_new_tokens=512\n)\n</code></pre>"},{"location":"examples/#vllm","title":"vLLM","text":"<p>You can also load AWQ models in vLLM.</p> <pre><code>import asyncio\nfrom transformers import AutoTokenizer, PreTrainedTokenizer\nfrom vllm import AsyncLLMEngine, SamplingParams, AsyncEngineArgs\n\nmodel_path = \"casperhansen/mixtral-instruct-awq\"\n\n# prompting\nprompt = \"You're standing on the surface of the Earth. \"\\\n         \"You walk one mile south, one mile west and one mile north. \"\\\n         \"You end up exactly where you started. Where are you?\",\n\nprompt_template = \"[INST] {prompt} [/INST]\"\n\n# sampling params\nsampling_params = SamplingParams(\n    repetition_penalty=1.1,\n    temperature=0.8,\n    max_tokens=512\n)\n\n# tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# async engine args for streaming\nengine_args = AsyncEngineArgs(\n    model=model_path,\n    quantization=\"awq\",\n    dtype=\"float16\",\n    max_model_len=512,\n    enforce_eager=True,\n    disable_log_requests=True,\n    disable_log_stats=True,\n)\n\nasync def generate(model: AsyncLLMEngine, tokenizer: PreTrainedTokenizer):\n    tokens = tokenizer(prompt_template.format(prompt=prompt)).input_ids\n\n    outputs = model.generate(\n        prompt=prompt,\n        sampling_params=sampling_params,\n        request_id=1,\n        prompt_token_ids=tokens,\n    )\n\n    print(\"\\n** Starting generation!\\n\")\n    last_index = 0\n\n    async for output in outputs:\n        print(output.outputs[0].text[last_index:], end=\"\", flush=True)\n        last_index = len(output.outputs[0].text)\n\n    print(\"\\n\\n** Finished generation!\\n\")\n\nif __name__ == '__main__':\n    model = AsyncLLMEngine.from_engine_args(engine_args)\n    asyncio.run(generate(model, tokenizer))\n</code></pre>"},{"location":"examples/#llava-multimodal","title":"LLaVa (multimodal)","text":"<p>AutoAWQ also supports the LLaVa model. You simply need to load an  AutoProcessor to process the prompt and image to generate inputs for the AWQ model.</p> <pre><code>import torch\nimport requests\nfrom PIL import Image\nfrom awq import AutoAWQForCausalLM\nfrom transformers import AutoProcessor, TextStreamer\n\n# Load model\nquant_path = \"casperhansen/llama3-llava-next-8b-awq\"\nmodel = AutoAWQForCausalLM.from_quantized(quant_path)\nprocessor = AutoProcessor.from_pretrained(quant_path)\nstreamer = TextStreamer(processor, skip_prompt=True)\n\n# Define prompt\nprompt = \"\"\"\\\n&lt;|im_start|&gt;system\\nAnswer the questions.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\\n&lt;image&gt;\\nWhat is shown in this image?&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n\"\"\"\n\n# Define image\nurl = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\n# Load inputs\ninputs = processor(prompt, image, return_tensors='pt').to(0, torch.float16)\n\ngeneration_output = model.generate(\n    **inputs,\n    max_new_tokens=512,\n    streamer=streamer\n)\n</code></pre>"},{"location":"examples/#qwen2-vl","title":"Qwen2 VL","text":"<p>Below is an example of how to run inference using Qwen2 VL.</p> <pre><code>from awq import AutoAWQForCausalLM\nfrom awq.utils.qwen_vl_utils import process_vision_info\nfrom transformers import AutoProcessor, TextStreamer\n\n# Load model\nquant_path = \"Qwen/Qwen2-VL-7B-Instruct-AWQ\"\nmodel = AutoAWQForCausalLM.from_quantized(quant_path)\nprocessor = AutoProcessor.from_pretrained(quant_path)\nstreamer = TextStreamer(processor, skip_prompt=True)\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n\n# Load inputs\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\ngeneration_output = model.generate(\n    **inputs,\n    max_new_tokens=512,\n    streamer=streamer\n)\n</code></pre>"},{"location":"reference/","title":"Auto and Base model classes in AutoAWQ","text":"<p>View the documentation of the main classes of AutoAWQ models below.</p>"},{"location":"reference/#awq.models.auto.AutoAWQForCausalLM","title":"awq.models.auto.AutoAWQForCausalLM","text":"<pre><code>AutoAWQForCausalLM()\n</code></pre> Source code in <code>awq/models/auto.py</code> <pre><code>def __init__(self):\n    raise EnvironmentError(\n        \"You must instantiate AutoAWQForCausalLM with\\n\"\n        \"AutoAWQForCausalLM.from_quantized or AutoAWQForCausalLM.from_pretrained\"\n    )\n</code></pre>"},{"location":"reference/#awq.models.auto.AutoAWQForCausalLM.from_pretrained","title":"from_pretrained  <code>classmethod</code>","text":"<pre><code>from_pretrained(model_path, torch_dtype='auto', trust_remote_code=True, safetensors=True, device_map=None, download_kwargs=None, low_cpu_mem_usage=True, use_cache=False, **model_init_kwargs)\n</code></pre> Source code in <code>awq/models/auto.py</code> <pre><code>@classmethod\ndef from_pretrained(\n    self,\n    model_path,\n    torch_dtype=\"auto\",\n    trust_remote_code=True,\n    safetensors=True,\n    device_map=None,\n    download_kwargs=None,\n    low_cpu_mem_usage=True,\n    use_cache=False,\n    **model_init_kwargs,\n) -&gt; BaseAWQForCausalLM:\n    model_type = check_and_get_model_type(\n        model_path, trust_remote_code, **model_init_kwargs\n    )\n    return AWQ_CAUSAL_LM_MODEL_MAP[model_type].from_pretrained(\n        model_path,\n        model_type,\n        torch_dtype=torch_dtype,\n        trust_remote_code=trust_remote_code,\n        safetensors=safetensors,\n        device_map=device_map,\n        download_kwargs=download_kwargs,\n        low_cpu_mem_usage=low_cpu_mem_usage,\n        use_cache=use_cache,\n        **model_init_kwargs,\n    )\n</code></pre>"},{"location":"reference/#awq.models.auto.AutoAWQForCausalLM.from_quantized","title":"from_quantized  <code>classmethod</code>","text":"<pre><code>from_quantized(quant_path, quant_filename='', max_seq_len=2048, trust_remote_code=True, fuse_layers=True, use_exllama=False, use_exllama_v2=False, use_ipex=False, batch_size=1, safetensors=True, device_map='balanced', max_memory=None, offload_folder=None, download_kwargs=None, **config_kwargs)\n</code></pre> Source code in <code>awq/models/auto.py</code> <pre><code>@classmethod\ndef from_quantized(\n    self,\n    quant_path,\n    quant_filename=\"\",\n    max_seq_len=2048,\n    trust_remote_code=True,\n    fuse_layers=True,\n    use_exllama=False,\n    use_exllama_v2=False,\n    use_ipex=False,\n    batch_size=1,\n    safetensors=True,\n    device_map=\"balanced\",\n    max_memory=None,\n    offload_folder=None,\n    download_kwargs=None,\n    **config_kwargs,\n) -&gt; BaseAWQForCausalLM:\n    os.environ[\"AWQ_BATCH_SIZE\"] = str(batch_size)\n    model_type = check_and_get_model_type(quant_path, trust_remote_code)\n\n    if config_kwargs.get(\"max_new_tokens\") is not None:\n        max_seq_len = config_kwargs[\"max_new_tokens\"]\n        logging.warning(\n            \"max_new_tokens argument is deprecated... gracefully \"\n            \"setting max_seq_len=max_new_tokens.\"\n        )\n\n    return AWQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized(\n        quant_path,\n        model_type,\n        quant_filename,\n        max_seq_len,\n        trust_remote_code=trust_remote_code,\n        fuse_layers=fuse_layers,\n        use_exllama=use_exllama,\n        use_exllama_v2=use_exllama_v2,\n        use_ipex=use_ipex,\n        safetensors=safetensors,\n        device_map=device_map,\n        max_memory=max_memory,\n        offload_folder=offload_folder,\n        download_kwargs=download_kwargs,\n        **config_kwargs,\n    )\n</code></pre>"},{"location":"reference/#awq.models.base.BaseAWQForCausalLM","title":"awq.models.base.BaseAWQForCausalLM","text":"<pre><code>BaseAWQForCausalLM(model, model_type, is_quantized, config, quant_config, processor)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>The base model for all AutoAWQ models.</p> PARAMETER DESCRIPTION <code>model</code> <p>The pretrained or quantized model.</p> <p> TYPE: <code>PreTrainedModel</code> </p> <code>model_type</code> <p>The model type, found in config.json.</p> <p> TYPE: <code>str</code> </p> <code>is_quantized</code> <p>Indicates if the current model is quantized.</p> <p> TYPE: <code>bool</code> </p> <code>config</code> <p>The config of the model.</p> <p> TYPE: <code>PretrainedConfig</code> </p> <code>quant_config</code> <p>The quantization config of the model.</p> <p> TYPE: <code>AwqConfig</code> </p> <code>processor</code> <p>An optional processor, e.g. for vision models.</p> <p> TYPE: <code>BaseImageProcessor</code> </p> Source code in <code>awq/models/base.py</code> <pre><code>def __init__(\n    self,\n    model: Annotated[PreTrainedModel, Doc(\"The pretrained or quantized model.\")],\n    model_type: Annotated[str, Doc(\"The model type, found in config.json.\")],\n    is_quantized: Annotated[\n        bool, Doc(\"Indicates if the current model is quantized.\")\n    ],\n    config: Annotated[PretrainedConfig, Doc(\"The config of the model.\")],\n    quant_config: Annotated[\n        AwqConfig, Doc(\"The quantization config of the model.\")\n    ],\n    processor: Annotated[\n        BaseImageProcessor, Doc(\"An optional processor, e.g. for vision models.\")\n    ],\n):\n    \"\"\"The base model for all AutoAWQ models.\"\"\"\n    super().__init__()\n    self.model: PreTrainedModel = model\n    self.model_type: str = model_type\n    self.is_quantized: bool = is_quantized\n    self.search_result = None\n    self.config: PretrainedConfig = config\n    self.quant_config: AwqConfig = quant_config\n    self.processor: ProcessorMixin = processor\n</code></pre>"},{"location":"reference/#awq.models.base.BaseAWQForCausalLM.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = model\n</code></pre>"},{"location":"reference/#awq.models.base.BaseAWQForCausalLM.model_type","title":"model_type  <code>instance-attribute</code>","text":"<pre><code>model_type = model_type\n</code></pre>"},{"location":"reference/#awq.models.base.BaseAWQForCausalLM.is_quantized","title":"is_quantized  <code>instance-attribute</code>","text":"<pre><code>is_quantized = is_quantized\n</code></pre>"},{"location":"reference/#awq.models.base.BaseAWQForCausalLM.search_result","title":"search_result  <code>instance-attribute</code>","text":"<pre><code>search_result = None\n</code></pre>"},{"location":"reference/#awq.models.base.BaseAWQForCausalLM.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config\n</code></pre>"},{"location":"reference/#awq.models.base.BaseAWQForCausalLM.quant_config","title":"quant_config  <code>instance-attribute</code>","text":"<pre><code>quant_config = quant_config\n</code></pre>"},{"location":"reference/#awq.models.base.BaseAWQForCausalLM.processor","title":"processor  <code>instance-attribute</code>","text":"<pre><code>processor = processor\n</code></pre>"},{"location":"reference/#awq.models.base.BaseAWQForCausalLM.to","title":"to","text":"<pre><code>to(device)\n</code></pre> <p>A utility function for moving the model to a device.</p> PARAMETER DESCRIPTION <code>device</code> <p>The device to move your model to.</p> <p> TYPE: <code>str</code> </p> Source code in <code>awq/models/base.py</code> <pre><code>def to(self, device: Annotated[str, Doc(\"The device to move your model to.\")]):\n    \"\"\"A utility function for moving the model to a device.\"\"\"\n    return self.model.to(device)\n</code></pre>"},{"location":"reference/#awq.models.base.BaseAWQForCausalLM.forward","title":"forward","text":"<pre><code>forward(*args, **kwargs)\n</code></pre> <p>A forward function that mimics the torch forward.</p> Source code in <code>awq/models/base.py</code> <pre><code>def forward(self, *args, **kwargs):\n    \"\"\"A forward function that mimics the torch forward.\"\"\"\n    return self.model(*args, **kwargs)\n</code></pre>"},{"location":"reference/#awq.models.base.BaseAWQForCausalLM.generate","title":"generate","text":"<pre><code>generate(*args, **kwargs)\n</code></pre> <p>A generate function that mimics the HF generate function.</p> Source code in <code>awq/models/base.py</code> <pre><code>def generate(self, *args, **kwargs):\n    \"\"\"A generate function that mimics the HF generate function.\"\"\"\n    with torch.inference_mode():\n        return self.model.generate(*args, **kwargs)\n</code></pre>"},{"location":"reference/#awq.models.base.BaseAWQForCausalLM.quantize","title":"quantize","text":"<pre><code>quantize(tokenizer=None, quant_config={}, calib_data='pileval', split='train', text_column='text', duo_scaling=True, export_compatible=False, apply_clip=True, n_parallel_calib_samples=None, max_calib_samples=128, max_calib_seq_len=512, max_chunk_memory=1024 * 1024 * 1024, quantizer_cls=AwqQuantizer, **kwargs)\n</code></pre> <p>The main quantization function that you can use to quantize your model.</p> <p>Example:</p> <pre><code>from awq import AutoAWQForCausalLM\nfrom transformers import AutoTokenizer\n\nmodel_path = \"...\"\nmodel = AutoAWQForCausalLM.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\nquant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\nmodel.quantize(tokenizer, quant_config)\n</code></pre> PARAMETER DESCRIPTION <code>tokenizer</code> <p>The tokenizer to use for quantization.</p> <p> TYPE: <code>PreTrainedTokenizer</code> DEFAULT: <code>None</code> </p> <code>quant_config</code> <p>The quantization config you want to use.</p> <p> TYPE: <code>Dict</code> DEFAULT: <code>{}</code> </p> <code>calib_data</code> <p>The calibration dataset. Either a string pointing to Huggingface or a list of preloaded examples.</p> <p> TYPE: <code>Union[str, List[str]]</code> DEFAULT: <code>'pileval'</code> </p> <code>split</code> <p>The split of calib_data.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'train'</code> </p> <code>text_column</code> <p>The text column of calib_data.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'text'</code> </p> <code>duo_scaling</code> <p>Whether to scale using both w/x or just x.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>export_compatible</code> <p>This argument avoids real quantization by only applying the scales without quantizing down to FP16.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>apply_clip</code> <p>Whether to apply clipping to the model during quantization. Some models may perform better with this set to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>n_parallel_calib_samples</code> <p>The number of parallel samples to run through the model. A high number of parallel samples can result in OOM during quantization if max_calib_samples is high enough. If None, runs through all samples at the same time. You can set this to a low number for more memory efficient quantization.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>max_calib_samples</code> <p>The maximum number of samples to run through the model.</p> <p> TYPE: <code>int</code> DEFAULT: <code>128</code> </p> <code>max_calib_seq_len</code> <p>The maximum sequence length of the calibration dataset. Discard samples greater than max_calib_seq_len.</p> <p> TYPE: <code>int</code> DEFAULT: <code>512</code> </p> <code>max_chunk_memory</code> <p>The loss computation and per-channel mean is optimized into chunked computations. Adjust this parameter to increase or decrease memory usage for these computations. Default is 1GB (1024 * 1024 * 1024).</p> <p> TYPE: <code>int</code> DEFAULT: <code>1024 * 1024 * 1024</code> </p> <code>quantizer_cls</code> <p>If you want to customize the quantization class, you can use AwqQuantizer as a base class.</p> <p> TYPE: <code>AwqQuantizer</code> DEFAULT: <code>AwqQuantizer</code> </p> Source code in <code>awq/models/base.py</code> <pre><code>@torch.no_grad()\ndef quantize(\n    self,\n    tokenizer: Annotated[\n        PreTrainedTokenizer, Doc(\"The tokenizer to use for quantization.\")\n    ] = None,\n    quant_config: Annotated[\n        Dict, Doc(\"The quantization config you want to use.\")\n    ] = {},\n    calib_data: Annotated[\n        Union[str, List[str]],\n        Doc(\n            \"The calibration dataset. Either a string pointing to Huggingface or a list of preloaded examples.\"\n        ),\n    ] = \"pileval\",\n    split: Annotated[str, Doc(\"The split of calib_data.\")] = \"train\",\n    text_column: Annotated[str, Doc(\"The text column of calib_data.\")] = \"text\",\n    duo_scaling: Annotated[\n        bool, Doc(\"Whether to scale using both w/x or just x.\")\n    ] = True,\n    export_compatible: Annotated[\n        bool,\n        Doc(\n            \"This argument avoids real quantization by only applying the scales without quantizing down to FP16.\"\n        ),\n    ] = False,\n    apply_clip: Annotated[\n        bool,\n        Doc(\n            \"Whether to apply clipping to the model during quantization. Some models may perform better with this set to False.\"\n        ),\n    ] = True,\n    n_parallel_calib_samples: Annotated[\n        int,\n        Doc(\n            \"The number of parallel samples to run through the model. \"\n            \"A high number of parallel samples can result in OOM during quantization if max_calib_samples is high enough. \"\n            \"If None, runs through all samples at the same time. \"\n            \"You can set this to a low number for more memory efficient quantization.\"\n        ),\n    ] = None,\n    max_calib_samples: Annotated[\n        int, Doc(\"The maximum number of samples to run through the model.\")\n    ] = 128,\n    max_calib_seq_len: Annotated[\n        int,\n        Doc(\n            \"The maximum sequence length of the calibration dataset. Discard samples greater than max_calib_seq_len.\"\n        ),\n    ] = 512,\n    max_chunk_memory: Annotated[\n        int,\n        Doc(\n            \"The loss computation and per-channel mean is optimized into chunked computations.\"\n            \" Adjust this parameter to increase or decrease memory usage for these computations.\"\n            \" Default is 1GB (1024 * 1024 * 1024).\"\n        ),\n    ] = 1024\n    * 1024\n    * 1024,\n    quantizer_cls: Annotated[\n        AwqQuantizer,\n        Doc(\"If you want to customize the quantization class, you can use AwqQuantizer as a base class.\")\n    ] = AwqQuantizer,\n    **kwargs,\n):\n    \"\"\"\n    The main quantization function that you can use to quantize your model.\n\n    Example:\n\n    ```python\n    from awq import AutoAWQForCausalLM\n    from transformers import AutoTokenizer\n\n    model_path = \"...\"\n    model = AutoAWQForCausalLM.from_pretrained(model_path)\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n    quant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\n    model.quantize(tokenizer, quant_config)\n    ```\n    \"\"\"\n    self.quant_config: AwqConfig = AwqConfig.from_dict(quant_config)\n\n    if hasattr(self, \"modules_to_not_convert\"):\n        self.quant_config.modules_to_not_convert = self.modules_to_not_convert\n\n    self.quantizer = quantizer_cls(\n        self,\n        self.model,\n        tokenizer,\n        self.quant_config.w_bit,\n        self.quant_config.q_group_size,\n        self.quant_config.zero_point,\n        self.quant_config.version,\n        calib_data,\n        split,\n        text_column,\n        duo_scaling,\n        modules_to_not_convert=self.quant_config.modules_to_not_convert,\n        export_compatible=export_compatible,\n        apply_clip=apply_clip,\n        n_parallel_calib_samples=n_parallel_calib_samples,\n        max_calib_samples=max_calib_samples,\n        max_calib_seq_len=max_calib_seq_len,\n        max_chunk_memory=max_chunk_memory,\n        **kwargs,\n    )\n    self.quantizer.quantize()\n\n    self.is_quantized = True\n</code></pre>"},{"location":"reference/#awq.models.base.BaseAWQForCausalLM.pack","title":"pack","text":"<pre><code>pack()\n</code></pre> <p>A utility function for the following scenario. Note that save_quantized will overwrite existing weights if you use the same quant_path.</p> <p>Example:</p> <pre><code>model.quantize(\n    tokenizer,\n    quant_config=quant_config,\n    export_compatible=True\n)\nmodel.save_quantized(...)  # produces GGUF/other compat weights\nmodel.pack(...) # makes the model CUDA compat\nmodel.save_quantized(...)  # produces CUDA compat weights\n</code></pre> Source code in <code>awq/models/base.py</code> <pre><code>@torch.no_grad()\ndef pack(self):\n    \"\"\"\n    A utility function for the following scenario. Note that save_quantized will\n    overwrite existing weights if you use the same quant_path.\n\n    Example:\n\n    ```python\n    model.quantize(\n        tokenizer,\n        quant_config=quant_config,\n        export_compatible=True\n    )\n    model.save_quantized(...)  # produces GGUF/other compat weights\n    model.pack(...) # makes the model CUDA compat\n    model.save_quantized(...)  # produces CUDA compat weights\n    ```\n    \"\"\"\n    self.quantizer.pack()\n</code></pre>"},{"location":"reference/#awq.models.base.BaseAWQForCausalLM.fuse_layers","title":"fuse_layers  <code>staticmethod</code>","text":"<pre><code>fuse_layers(model)\n</code></pre> Source code in <code>awq/models/base.py</code> <pre><code>@staticmethod\ndef fuse_layers(model):\n    pass\n</code></pre>"},{"location":"reference/#awq.models.base.BaseAWQForCausalLM.save_quantized","title":"save_quantized","text":"<pre><code>save_quantized(save_dir, safetensors=True, shard_size='5GB')\n</code></pre> PARAMETER DESCRIPTION <code>save_dir</code> <p>The directory to save your model to.</p> <p> TYPE: <code>str</code> </p> <code>safetensors</code> <p>Whether to save the model as safetensors or torch files.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>shard_size</code> <p>The shard size for sharding large models into multiple chunks.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'5GB'</code> </p> Source code in <code>awq/models/base.py</code> <pre><code>def save_quantized(\n    self,\n    save_dir: Annotated[str, Doc(\"The directory to save your model to.\")],\n    safetensors: Annotated[\n        bool, Doc(\"Whether to save the model as safetensors or torch files.\")\n    ] = True,\n    shard_size: Annotated[\n        str, Doc(\"The shard size for sharding large models into multiple chunks.\")\n    ] = \"5GB\",\n):\n    save_dir = save_dir[:-1] if save_dir[-1] == \"/\" else save_dir\n\n    # Save model\n    class EmptyModule(nn.Module):\n        def __init__(self):\n            super(EmptyModule, self).__init__()\n\n        def forward(self, x):\n            return x\n\n    # Save model and config files with empty state dict\n    self.model.config.quantization_config = self.quant_config.to_transformers_dict()\n    self.model.generation_config.do_sample = True\n    self.model.save_pretrained(save_dir, state_dict=EmptyModule().state_dict())\n\n    # Vision transformers have a processor\n    if self.processor is not None:\n        self.processor.save_pretrained(save_dir)\n\n    # Remove empty state dict\n    default_paths = [\n        f\"{save_dir}/model.safetensors\",\n        f\"{save_dir}/pytorch_model.bin\",\n    ]\n    for path in default_paths:\n        if os.path.exists(path):\n            os.remove(path)\n\n    save_torch_state_dict(\n        state_dict=self.model.state_dict(),\n        save_directory=save_dir,\n        max_shard_size=shard_size,\n        safe_serialization=safetensors,\n        force_contiguous=True,\n        shared_tensors_to_discard=self.model._tied_weights_keys,\n    )\n</code></pre>"},{"location":"reference/#awq.models.base.BaseAWQForCausalLM.from_pretrained","title":"from_pretrained  <code>classmethod</code>","text":"<pre><code>from_pretrained(model_path, model_type, torch_dtype=float16, trust_remote_code=True, safetensors=True, device_map='auto', download_kwargs=None, low_cpu_mem_usage=True, use_cache=False, **model_init_kwargs)\n</code></pre> <p>A method for initialization of pretrained models, usually in FP16.</p> PARAMETER DESCRIPTION <code>model_path</code> <p>A Huggingface path or local path to a model.</p> <p> TYPE: <code>str</code> </p> <code>model_type</code> <p>The model type, loaded from config.json.</p> <p> TYPE: <code>str</code> </p> <code>torch_dtype</code> <p>The dtype to load the model as. May not work with other values than float16.</p> <p> TYPE: <code>dtype</code> DEFAULT: <code>float16</code> </p> <code>trust_remote_code</code> <p>Useful for Huggingface repositories that have not been integrated into transformers yet.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>safetensors</code> <p>Whether to download/load safetensors instead of torch weights.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>device_map</code> <p>A device map that will be passed onto the model loading method from transformers.</p> <p> TYPE: <code>Union[str, Dict]</code> DEFAULT: <code>'auto'</code> </p> <code>download_kwargs</code> <p>Used for configure download model</p> <p> TYPE: <code>Dict</code> DEFAULT: <code>None</code> </p> <code>low_cpu_mem_usage</code> <p>Use low_cpu_mem_usage when loading from transformers.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>use_cache</code> <p>Use use_cache argument in transformers</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>**model_init_kwargs</code> <p>Additional kwargs that are passed to the model during initialization.</p> <p> TYPE: <code>Dict</code> DEFAULT: <code>{}</code> </p> Source code in <code>awq/models/base.py</code> <pre><code>@classmethod\ndef from_pretrained(\n    self,\n    model_path: Annotated[str, Doc(\"A Huggingface path or local path to a model.\")],\n    model_type: Annotated[str, Doc(\"The model type, loaded from config.json.\")],\n    torch_dtype: Annotated[\n        torch.dtype,\n        Doc(\n            \"The dtype to load the model as. May not work with other values than float16.\"\n        ),\n    ] = torch.float16,\n    trust_remote_code: Annotated[\n        bool,\n        Doc(\n            \"Useful for Huggingface repositories that have not been integrated into transformers yet.\"\n        ),\n    ] = True,\n    safetensors: Annotated[\n        bool, Doc(\"Whether to download/load safetensors instead of torch weights.\")\n    ] = True,\n    device_map: Annotated[\n        Union[str, Dict],\n        Doc(\n            \"A device map that will be passed onto the model loading method from transformers.\"\n        ),\n    ] = \"auto\",\n    download_kwargs: Annotated[\n        Dict,\n        Doc(\"Used for configure download model\"),\n    ] = None,\n    low_cpu_mem_usage: Annotated[\n        bool,\n        Doc(\"Use low_cpu_mem_usage when loading from transformers.\")\n    ] = True,\n    use_cache: Annotated[\n        bool,\n        Doc(\"Use use_cache argument in transformers\")\n    ] = False,\n    **model_init_kwargs: Annotated[\n        Dict,\n        Doc(\n            \"Additional kwargs that are passed to the model during initialization.\"\n        ),\n    ],\n):\n    \"\"\"A method for initialization of pretrained models, usually in FP16.\"\"\"\n    # Get weights path and quant config\n    model_weights_path, config, quant_config = self._load_config(\n        self,\n        model_path,\n        \"\",\n        safetensors,\n        trust_remote_code=trust_remote_code,\n        download_kwargs=download_kwargs,\n    )\n\n    target_cls_name = TRANSFORMERS_AUTO_MAPPING_DICT[config.model_type]\n    target_cls = getattr(transformers, target_cls_name)\n\n    processor = None\n    if target_cls_name == \"AutoModelForVision2Seq\" or target_cls_name == \"AutoModelForTextToWaveform\":\n        processor = AutoProcessor.from_pretrained(model_weights_path)\n    if model_init_kwargs.get(\"low_cpu_mem_usage\") is None:\n        model_init_kwargs[\"low_cpu_mem_usage\"] = low_cpu_mem_usage\n    if model_init_kwargs.get(\"use_cache\") is None and not ((target_cls_name == \"AutoModelForVision2Seq\") or (target_cls_name == \"AutoModelForTextToWaveform\")):\n        model_init_kwargs[\"use_cache\"] = use_cache\n\n    # If not quantized, must load with AutoModelForCausalLM\n    model = target_cls.from_pretrained(\n        model_weights_path,\n        trust_remote_code=trust_remote_code,\n        torch_dtype=torch_dtype,\n        use_safetensors=safetensors,\n        device_map=device_map,\n        **model_init_kwargs,\n    )\n\n    model.eval()\n\n    return self(\n        model,\n        model_type,\n        is_quantized=False,\n        config=config,\n        quant_config=quant_config,\n        processor=processor,\n    )\n</code></pre>"},{"location":"reference/#awq.models.base.BaseAWQForCausalLM.from_quantized","title":"from_quantized  <code>classmethod</code>","text":"<pre><code>from_quantized(model_path, model_type, model_filename='', max_seq_len=None, torch_dtype=float16, trust_remote_code=True, safetensors=True, fuse_layers=True, use_exllama=False, use_exllama_v2=False, use_ipex=False, device_map='balanced', max_memory=None, offload_folder=None, download_kwargs=None, **config_kwargs)\n</code></pre> <p>A method for initialization of a quantized model, usually in INT4.</p> PARAMETER DESCRIPTION <code>model_path</code> <p>A Huggingface path or local path to a model.</p> <p> TYPE: <code>str</code> </p> <code>model_type</code> <p>The model type, loaded from config.json.</p> <p> TYPE: <code>str</code> </p> <code>model_filename</code> <p>Load a specific model's filename by specifying this argument.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>max_seq_len</code> <p>The maximum sequence cached sequence length of the model. Larger values may increase loading time and memory usage.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>torch_dtype</code> <p>The dtype to load the model as. May not work with other values than float16.</p> <p> TYPE: <code>dtype</code> DEFAULT: <code>float16</code> </p> <code>trust_remote_code</code> <p>Useful for Huggingface repositories that have not been integrated into transformers yet.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>safetensors</code> <p>Whether to download/load safetensors instead of torch weights.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>fuse_layers</code> <p>Whether to use fused/optimized combination of layers for increased speed.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>use_exllama</code> <p>Whether to map the weights to ExLlamaV1 kernels.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>use_exllama_v2</code> <p>Whether to map the weights to ExLlamaV2 kernels.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>use_ipex</code> <p>Whether to map the weights to ipex kernels for CPU and XPU device.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>device_map</code> <p>A device map that will be passed onto the model loading method from transformers.</p> <p> TYPE: <code>Union[str, Dict]</code> DEFAULT: <code>'balanced'</code> </p> <code>max_memory</code> <p>A dictionary device identifier to maximum memory which will be passed onto the model loading method from transformers. For example\uff1a{0: \"4GB\",1: \"10GB\"</p> <p> TYPE: <code>Dict[Union[int, str], Union[int, str]]</code> DEFAULT: <code>None</code> </p> <code>offload_folder</code> <p>The folder ot offload the model to.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>download_kwargs</code> <p>Used for configure download model</p> <p> TYPE: <code>Dict</code> DEFAULT: <code>None</code> </p> <code>**config_kwargs</code> <p>Additional kwargs that are passed to the config during initialization.</p> <p> TYPE: <code>Dict</code> DEFAULT: <code>{}</code> </p> Source code in <code>awq/models/base.py</code> <pre><code>@classmethod\ndef from_quantized(\n    self,\n    model_path: Annotated[str, Doc(\"A Huggingface path or local path to a model.\")],\n    model_type: Annotated[str, Doc(\"The model type, loaded from config.json.\")],\n    model_filename: Annotated[\n        str, Doc(\"Load a specific model's filename by specifying this argument.\")\n    ] = \"\",\n    max_seq_len: Annotated[\n        int,\n        Doc(\n            \"The maximum sequence cached sequence length of the model. Larger values may increase loading time and memory usage.\"\n        ),\n    ] = None,\n    torch_dtype: Annotated[\n        torch.dtype,\n        Doc(\n            \"The dtype to load the model as. May not work with other values than float16.\"\n        ),\n    ] = torch.float16,\n    trust_remote_code: Annotated[\n        bool,\n        Doc(\n            \"Useful for Huggingface repositories that have not been integrated into transformers yet.\"\n        ),\n    ] = True,\n    safetensors: Annotated[\n        bool, Doc(\"Whether to download/load safetensors instead of torch weights.\")\n    ] = True,\n    fuse_layers: Annotated[\n        bool,\n        Doc(\n            \"Whether to use fused/optimized combination of layers for increased speed.\"\n        ),\n    ] = True,\n    use_exllama: Annotated[\n        bool, Doc(\"Whether to map the weights to ExLlamaV1 kernels.\")\n    ] = False,\n    use_exllama_v2: Annotated[\n        bool, Doc(\"Whether to map the weights to ExLlamaV2 kernels.\")\n    ] = False,\n    use_ipex: Annotated[\n        bool, Doc(\"Whether to map the weights to ipex kernels for CPU and XPU device.\")\n    ] = False,\n    device_map: Annotated[\n        Union[str, Dict],\n        Doc(\n            \"A device map that will be passed onto the model loading method from transformers.\"\n        ),\n    ] = \"balanced\",\n    max_memory: Annotated[\n        Dict[Union[int, str], Union[int, str]],\n        Doc(\n            'A dictionary device identifier to maximum memory which will be passed onto the model loading method from transformers. For example\uff1a{0: \"4GB\",1: \"10GB\"'\n        ),\n    ] = None,\n    offload_folder: Annotated[\n        str,\n        Doc(\"The folder ot offload the model to.\"),\n    ] = None,\n    download_kwargs: Annotated[\n        Dict,\n        Doc(\"Used for configure download model\"),\n    ] = None,\n    **config_kwargs: Annotated[\n        Dict,\n        Doc(\n            \"Additional kwargs that are passed to the config during initialization.\"\n        ),\n    ],\n):\n    \"\"\"A method for initialization of a quantized model, usually in INT4.\"\"\"\n    # [STEP 1-2] Load weights path and configs\n    model_weights_path, config, quant_config = self._load_config(\n        self,\n        model_path,\n        model_filename,\n        safetensors,\n        trust_remote_code,\n        max_seq_len=max_seq_len,\n        download_kwargs=download_kwargs,\n        **config_kwargs,\n    )\n\n    target_cls_name = TRANSFORMERS_AUTO_MAPPING_DICT[config.model_type]\n    target_cls = getattr(transformers, target_cls_name)\n\n    # [STEP 3] Load model\n    with init_empty_weights():\n        model = target_cls.from_config(\n            config=config,\n            torch_dtype=torch_dtype,\n            trust_remote_code=trust_remote_code,\n        )\n\n    best_device = get_best_device()\n    if best_device == \"cpu\" or (best_device == \"xpu:0\" and not triton_available):\n        use_ipex = True\n    if use_ipex and not ipex_available:\n        raise ImportError(\n            \"Please install intel_extension_for_pytorch with \"\n            \"`pip install intel_extension_for_pytorch` for 'ipex' kernel!\"\n        )\n    # Prepare WQLinear layers, replace nn.Linear\n    self._load_quantized_modules(\n        self,\n        model,\n        quant_config,\n        quant_config.version,\n        use_exllama=use_exllama,\n        use_exllama_v2=use_exllama_v2,\n        use_ipex=use_ipex,\n    )\n\n    model.tie_weights()\n\n    # loads the weights into modules and distributes\n    # across available devices automatically\n    load_checkpoint_and_dispatch(\n        model,\n        checkpoint=model_weights_path,\n        device_map=device_map,\n        max_memory=max_memory,\n        no_split_module_classes=[self.layer_type],\n        offload_folder=offload_folder,\n        dtype=torch_dtype,\n    )\n\n    # Dispath to devices\n    awq_ext, msg = try_import(\"awq_ext\")\n    if fuse_layers:\n        if best_device in [\"mps\", \"cuda:0\"] and awq_ext is None:\n            warnings.warn(\"Skipping fusing modules because AWQ extension is not installed.\" + msg)\n        else:\n            self.fuse_layers(model)\n\n    if use_ipex:\n        # repack qweight to match the ipex kernel.\n        model = ipex_post_init(model)\n    elif quant_config.version == \"marlin\":\n        model = marlin_post_init(model)\n    elif use_exllama:\n        # creates q4 handle\n        model = exllama_post_init(model)\n    elif use_exllama_v2:\n        # creates q4 handle and allocates scratch spaces wrt max_input_len and max_batch_size\n        model = exllamav2_post_init(\n            model,\n            max_input_len=max_seq_len or 2048,\n            max_batch_size=int(os.getenv(\"AWQ_BATCH_SIZE\", 1)),\n        )\n\n    model.eval()\n\n    return self(\n        model,\n        model_type,\n        is_quantized=True,\n        config=config,\n        quant_config=quant_config,\n        processor=None,\n    )\n</code></pre>"}]}