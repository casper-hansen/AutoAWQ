
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="..">
      
      
        <link rel="next" href="../reference/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.13">
    
    
      
        <title>Examples - AutoAWQ</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="amber">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#examples" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="AutoAWQ" class="md-header__button md-logo" aria-label="AutoAWQ" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            AutoAWQ
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Examples
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="amber"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 2a7 7 0 0 0-7 7c0 2.38 1.19 4.47 3 5.74V17a1 1 0 0 0 1 1h6a1 1 0 0 0 1-1v-2.26c1.81-1.27 3-3.36 3-5.74a7 7 0 0 0-7-7M9 21a1 1 0 0 0 1 1h4a1 1 0 0 0 1-1v-1H9z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="amber"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 2a7 7 0 0 1 7 7c0 2.38-1.19 4.47-3 5.74V17a1 1 0 0 1-1 1H9a1 1 0 0 1-1-1v-2.26C6.19 13.47 5 11.38 5 9a7 7 0 0 1 7-7M9 21v-1h6v1a1 1 0 0 1-1 1h-4a1 1 0 0 1-1-1m3-17a5 5 0 0 0-5 5c0 2.05 1.23 3.81 3 4.58V16h4v-2.42c1.77-.77 3-2.53 3-4.58a5 5 0 0 0-5-5"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/casper-hansen/AutoAWQ" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1M480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2m-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3m-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1"/></svg>
  </div>
  <div class="md-source__repository">
    casper-hansen/AutoAWQ
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href=".." class="md-tabs__link">
        
  
  
    
  
  AutoAWQ

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    <li class="md-tabs__item md-tabs__item--active">
      <a href="./" class="md-tabs__link">
        
  
  
    
  
  Examples

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../reference/" class="md-tabs__link">
          
  
  
    
  
  Reference

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="AutoAWQ" class="md-nav__button md-logo" aria-label="AutoAWQ" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    AutoAWQ
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/casper-hansen/AutoAWQ" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1M480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2m-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3m-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1"/></svg>
  </div>
  <div class="md-source__repository">
    casper-hansen/AutoAWQ
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AutoAWQ
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Examples
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Examples
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#basic-quantization" class="md-nav__link">
    <span class="md-ellipsis">
      Basic Quantization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Basic Quantization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#custom-data" class="md-nav__link">
    <span class="md-ellipsis">
      Custom Data
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Custom Data">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#long-context-optimizing-quantization" class="md-nav__link">
    <span class="md-ellipsis">
      Long-context: Optimizing quantization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#coding-models" class="md-nav__link">
    <span class="md-ellipsis">
      Coding models
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vision-language-models" class="md-nav__link">
    <span class="md-ellipsis">
      Vision-Language Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gguf-export" class="md-nav__link">
    <span class="md-ellipsis">
      GGUF Export
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#custom-quantizer-qwen2-vl-example" class="md-nav__link">
    <span class="md-ellipsis">
      Custom Quantizer (Qwen2 VL Example)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#another-custom-quantizer-minicpm3-example" class="md-nav__link">
    <span class="md-ellipsis">
      Another Custom Quantizer (MiniCPM3 Example)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#basic-inference" class="md-nav__link">
    <span class="md-ellipsis">
      Basic Inference
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Basic Inference">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#inference-with-gpu" class="md-nav__link">
    <span class="md-ellipsis">
      Inference With GPU
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#inference-with-cpu" class="md-nav__link">
    <span class="md-ellipsis">
      Inference With CPU
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers" class="md-nav__link">
    <span class="md-ellipsis">
      Transformers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vllm" class="md-nav__link">
    <span class="md-ellipsis">
      vLLM
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llava-multimodal" class="md-nav__link">
    <span class="md-ellipsis">
      LLaVa (multimodal)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen2-vl" class="md-nav__link">
    <span class="md-ellipsis">
      Qwen2 VL
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../reference/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Reference
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#basic-quantization" class="md-nav__link">
    <span class="md-ellipsis">
      Basic Quantization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Basic Quantization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#custom-data" class="md-nav__link">
    <span class="md-ellipsis">
      Custom Data
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Custom Data">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#long-context-optimizing-quantization" class="md-nav__link">
    <span class="md-ellipsis">
      Long-context: Optimizing quantization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#coding-models" class="md-nav__link">
    <span class="md-ellipsis">
      Coding models
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vision-language-models" class="md-nav__link">
    <span class="md-ellipsis">
      Vision-Language Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gguf-export" class="md-nav__link">
    <span class="md-ellipsis">
      GGUF Export
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#custom-quantizer-qwen2-vl-example" class="md-nav__link">
    <span class="md-ellipsis">
      Custom Quantizer (Qwen2 VL Example)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#another-custom-quantizer-minicpm3-example" class="md-nav__link">
    <span class="md-ellipsis">
      Another Custom Quantizer (MiniCPM3 Example)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#basic-inference" class="md-nav__link">
    <span class="md-ellipsis">
      Basic Inference
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Basic Inference">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#inference-with-gpu" class="md-nav__link">
    <span class="md-ellipsis">
      Inference With GPU
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#inference-with-cpu" class="md-nav__link">
    <span class="md-ellipsis">
      Inference With CPU
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers" class="md-nav__link">
    <span class="md-ellipsis">
      Transformers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vllm" class="md-nav__link">
    <span class="md-ellipsis">
      vLLM
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llava-multimodal" class="md-nav__link">
    <span class="md-ellipsis">
      LLaVa (multimodal)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen2-vl" class="md-nav__link">
    <span class="md-ellipsis">
      Qwen2 VL
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="examples">Examples<a class="headerlink" href="#examples" title="Permanent link">&para;</a></h1>
<h2 id="basic-quantization">Basic Quantization<a class="headerlink" href="#basic-quantization" title="Permanent link">&para;</a></h2>
<p>AWQ performs zero point quantization down to a precision of 4-bit integers.
You can also specify other bit rates like 3-bit, but some of these options may lack kernels
for running inference.</p>
<p>Notes:</p>
<ul>
<li>Some models like Falcon is only compatible with group size 64.</li>
<li>To use Marlin, you must specify zero point as False and version as Marlin.</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">awq</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoAWQForCausalLM</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">model_path</span> <span class="o">=</span> <span class="s1">&#39;mistralai/Mistral-7B-Instruct-v0.2&#39;</span>
<span class="n">quant_path</span> <span class="o">=</span> <span class="s1">&#39;mistral-instruct-v0.2-awq&#39;</span>
<span class="n">quant_config</span> <span class="o">=</span> <span class="p">{</span> <span class="s2">&quot;zero_point&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="s2">&quot;q_group_size&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span> <span class="s2">&quot;w_bit&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;version&quot;</span><span class="p">:</span> <span class="s2">&quot;GEMM&quot;</span> <span class="p">}</span>

<span class="c1"># Load model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoAWQForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Quantize</span>
<span class="n">model</span><span class="o">.</span><span class="n">quantize</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">quant_config</span><span class="o">=</span><span class="n">quant_config</span><span class="p">)</span>

<span class="c1"># Save quantized model</span>
<span class="n">model</span><span class="o">.</span><span class="n">save_quantized</span><span class="p">(</span><span class="n">quant_path</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">quant_path</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Model is quantized and saved at &quot;</span><span class="si">{</span><span class="n">quant_path</span><span class="si">}</span><span class="s1">&quot;&#39;</span><span class="p">)</span>
</code></pre></div>
<h3 id="custom-data">Custom Data<a class="headerlink" href="#custom-data" title="Permanent link">&para;</a></h3>
<p>This includes an example function that loads either wikitext or dolly.
Note that currently all samples above 512 in length are discarded.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">awq</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoAWQForCausalLM</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">model_path</span> <span class="o">=</span> <span class="s1">&#39;lmsys/vicuna-7b-v1.5&#39;</span>
<span class="n">quant_path</span> <span class="o">=</span> <span class="s1">&#39;vicuna-7b-v1.5-awq&#39;</span>
<span class="n">quant_config</span> <span class="o">=</span> <span class="p">{</span> <span class="s2">&quot;zero_point&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="s2">&quot;q_group_size&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span> <span class="s2">&quot;w_bit&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;version&quot;</span><span class="p">:</span> <span class="s2">&quot;GEMM&quot;</span> <span class="p">}</span>

<span class="c1"># Load model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoAWQForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Define data loading methods</span>
<span class="k">def</span><span class="w"> </span><span class="nf">load_dolly</span><span class="p">():</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;databricks/databricks-dolly-15k&#39;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>

    <span class="c1"># concatenate data</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">concatenate_data</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;instruction&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;context&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;response&#39;</span><span class="p">]}</span>

    <span class="n">concatenated</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">concatenate_data</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">text</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">concatenated</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]]</span>

<span class="k">def</span><span class="w"> </span><span class="nf">load_wikitext</span><span class="p">():</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;wikitext&#39;</span><span class="p">,</span> <span class="s1">&#39;wikitext-2-raw-v1&#39;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">text</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="n">text</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="o">!=</span> <span class="s1">&#39;&#39;</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mi">20</span><span class="p">]</span>

<span class="c1"># Quantize</span>
<span class="n">model</span><span class="o">.</span><span class="n">quantize</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">quant_config</span><span class="o">=</span><span class="n">quant_config</span><span class="p">,</span> <span class="n">calib_data</span><span class="o">=</span><span class="n">load_wikitext</span><span class="p">())</span>

<span class="c1"># Save quantized model</span>
<span class="n">model</span><span class="o">.</span><span class="n">save_quantized</span><span class="p">(</span><span class="n">quant_path</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">quant_path</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Model is quantized and saved at &quot;</span><span class="si">{</span><span class="n">quant_path</span><span class="si">}</span><span class="s1">&quot;&#39;</span><span class="p">)</span>
</code></pre></div>
<h4 id="long-context-optimizing-quantization">Long-context: Optimizing quantization<a class="headerlink" href="#long-context-optimizing-quantization" title="Permanent link">&para;</a></h4>
<p>For this example, we will use HuggingFaceTB/cosmopedia-100k as it's a high-quality dataset and
we can filter directly on the number of tokens. We will use Qwen2 7B, one of the newer supported
models in AutoAWQ which is high-performing. The following example ran smoothly on a machine with
an RTX 4090 24 GB VRAM with 107 GB system RAM.</p>
<p>NOTE: Adjusting <code>n_parallel_calib_samples</code>, <code>max_calib_samples</code>, and <code>max_calib_seq_len</code> will help
avoid OOM when customizing your dataset.</p>
<ul>
<li>The AWQ algorithm is incredibly sample efficient, so <code>max_calib_samples</code> of 128-256 should be
sufficient to quantize a model. A higher number of samples may not be possible without significant
memory available or without further optimizing AWQ with a PR for disk offload.</li>
<li>When <code>n_parallel_calib_samples</code> is set to an integer, we offload to system RAM to save GPU VRAM.
This may cause OOM on your system if you have little memory available; we are looking to optimize
this further in future versions.</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">awq</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoAWQForCausalLM</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">model_path</span> <span class="o">=</span> <span class="s1">&#39;Qwen/Qwen2-7B-Instruct&#39;</span>
<span class="n">quant_path</span> <span class="o">=</span> <span class="s1">&#39;qwen2-7b-awq&#39;</span>
<span class="n">quant_config</span> <span class="o">=</span> <span class="p">{</span> <span class="s2">&quot;zero_point&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="s2">&quot;q_group_size&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span> <span class="s2">&quot;w_bit&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;version&quot;</span><span class="p">:</span> <span class="s2">&quot;GEMM&quot;</span> <span class="p">}</span>

<span class="c1"># Load model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoAWQForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">load_cosmopedia</span><span class="p">():</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;HuggingFaceTB/cosmopedia-100k&#39;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s2">&quot;text_token_length&quot;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">2048</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">[</span><span class="n">text</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]]</span>

<span class="c1"># Quantize</span>
<span class="n">model</span><span class="o">.</span><span class="n">quantize</span><span class="p">(</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">quant_config</span><span class="o">=</span><span class="n">quant_config</span><span class="p">,</span>
    <span class="n">calib_data</span><span class="o">=</span><span class="n">load_cosmopedia</span><span class="p">(),</span>
    <span class="n">n_parallel_calib_samples</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">max_calib_samples</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">max_calib_seq_len</span><span class="o">=</span><span class="mi">4096</span>
<span class="p">)</span>

<span class="c1"># Save quantized model</span>
<span class="n">model</span><span class="o">.</span><span class="n">save_quantized</span><span class="p">(</span><span class="n">quant_path</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">quant_path</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Model is quantized and saved at &quot;</span><span class="si">{</span><span class="n">quant_path</span><span class="si">}</span><span class="s1">&quot;&#39;</span><span class="p">)</span>
</code></pre></div>
<h4 id="coding-models">Coding models<a class="headerlink" href="#coding-models" title="Permanent link">&para;</a></h4>
<p>For this example, we will use deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct as it's an excellent coding model.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">tqdm</span><span class="w"> </span><span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">awq</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoAWQForCausalLM</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">model_path</span> <span class="o">=</span> <span class="s1">&#39;deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct&#39;</span>
<span class="n">quant_path</span> <span class="o">=</span> <span class="s1">&#39;deepseek-coder-v2-lite-instruct-awq&#39;</span>
<span class="n">quant_config</span> <span class="o">=</span> <span class="p">{</span> <span class="s2">&quot;zero_point&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="s2">&quot;q_group_size&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span> <span class="s2">&quot;w_bit&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;version&quot;</span><span class="p">:</span> <span class="s2">&quot;GEMM&quot;</span> <span class="p">}</span>

<span class="c1"># Load model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoAWQForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">load_openhermes_coding</span><span class="p">():</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;alvarobartt/openhermes-preferences-coding&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>

    <span class="n">samples</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
        <span class="n">responses</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">response</span><span class="p">[</span><span class="s2">&quot;role&quot;</span><span class="p">]</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="n">response</span><span class="p">[</span><span class="s2">&quot;content&quot;</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">response</span> <span class="ow">in</span> <span class="n">sample</span><span class="p">[</span><span class="s2">&quot;chosen&quot;</span><span class="p">]]</span>
        <span class="n">samples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">responses</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">samples</span>

<span class="c1"># Quantize</span>
<span class="n">model</span><span class="o">.</span><span class="n">quantize</span><span class="p">(</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">quant_config</span><span class="o">=</span><span class="n">quant_config</span><span class="p">,</span>
    <span class="n">calib_data</span><span class="o">=</span><span class="n">load_openhermes_coding</span><span class="p">(),</span>
    <span class="c1"># MODIFY these parameters if need be:</span>
    <span class="c1"># n_parallel_calib_samples=32,</span>
    <span class="c1"># max_calib_samples=128,</span>
    <span class="c1"># max_calib_seq_len=4096</span>
<span class="p">)</span>

<span class="c1"># Save quantized model</span>
<span class="n">model</span><span class="o">.</span><span class="n">save_quantized</span><span class="p">(</span><span class="n">quant_path</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">quant_path</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Model is quantized and saved at &quot;</span><span class="si">{</span><span class="n">quant_path</span><span class="si">}</span><span class="s1">&quot;&#39;</span><span class="p">)</span>
</code></pre></div>
<h3 id="vision-language-models">Vision-Language Models<a class="headerlink" href="#vision-language-models" title="Permanent link">&para;</a></h3>
<p>AutoAWQ supports a few vision-language models. So far, we support LLaVa 1.5 and LLaVa 1.6 (next).</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">awq</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoAWQForCausalLM</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">model_path</span> <span class="o">=</span> <span class="s1">&#39;llava-hf/llama3-llava-next-8b-hf&#39;</span>
<span class="n">quant_path</span> <span class="o">=</span> <span class="s1">&#39;llama3-llava-next-8b-awq&#39;</span>
<span class="n">quant_config</span> <span class="o">=</span> <span class="p">{</span> <span class="s2">&quot;zero_point&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="s2">&quot;q_group_size&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span> <span class="s2">&quot;w_bit&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;version&quot;</span><span class="p">:</span> <span class="s2">&quot;GEMM&quot;</span> <span class="p">}</span>

<span class="c1"># Load model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoAWQForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">model_path</span><span class="p">,</span> <span class="n">low_cpu_mem_usage</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Quantize</span>
<span class="n">model</span><span class="o">.</span><span class="n">quantize</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">quant_config</span><span class="o">=</span><span class="n">quant_config</span><span class="p">)</span>

<span class="c1"># Save quantized model</span>
<span class="n">model</span><span class="o">.</span><span class="n">save_quantized</span><span class="p">(</span><span class="n">quant_path</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">quant_path</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Model is quantized and saved at &quot;</span><span class="si">{</span><span class="n">quant_path</span><span class="si">}</span><span class="s1">&quot;&#39;</span><span class="p">)</span>
</code></pre></div>
<h3 id="gguf-export">GGUF Export<a class="headerlink" href="#gguf-export" title="Permanent link">&para;</a></h3>
<p>This computes AWQ scales and appliesthem to the model without running real quantization.
This keeps the quality of AWQ because theweights are applied but skips quantization
in order to make it compatible with other frameworks.</p>
<p>Step by step:</p>
<ul>
<li><code>quantize()</code>: Compute AWQ scales and apply them</li>
<li><code>save_pretrained()</code>: Saves a non-quantized model in FP16</li>
<li><code>convert.py</code>: Convert the Huggingface FP16 weights to GGUF FP16 weights</li>
<li><code>quantize</code>: Run GGUF quantization to get real quantized weights, in this case 4-bit.</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">subprocess</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">awq</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoAWQForCausalLM</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">model_path</span> <span class="o">=</span> <span class="s1">&#39;mistralai/Mistral-7B-v0.1&#39;</span>
<span class="n">quant_path</span> <span class="o">=</span> <span class="s1">&#39;mistral-awq&#39;</span>
<span class="n">llama_cpp_path</span> <span class="o">=</span> <span class="s1">&#39;/workspace/llama.cpp&#39;</span>
<span class="n">quant_config</span> <span class="o">=</span> <span class="p">{</span> <span class="s2">&quot;zero_point&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="s2">&quot;q_group_size&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span> <span class="s2">&quot;w_bit&quot;</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span> <span class="s2">&quot;version&quot;</span><span class="p">:</span> <span class="s2">&quot;GEMM&quot;</span> <span class="p">}</span>

<span class="c1"># Load model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoAWQForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Quantize</span>
<span class="c1"># NOTE: We avoid packing weights, so you cannot use this model in AutoAWQ</span>
<span class="c1"># after quantizing. The saved model is FP16 but has the AWQ scales applied.</span>
<span class="n">model</span><span class="o">.</span><span class="n">quantize</span><span class="p">(</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">quant_config</span><span class="o">=</span><span class="n">quant_config</span><span class="p">,</span>
    <span class="n">export_compatible</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="c1"># Save quantized model</span>
<span class="n">model</span><span class="o">.</span><span class="n">save_quantized</span><span class="p">(</span><span class="n">quant_path</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">quant_path</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Model is quantized and saved at &quot;</span><span class="si">{</span><span class="n">quant_path</span><span class="si">}</span><span class="s1">&quot;&#39;</span><span class="p">)</span>

<span class="c1"># GGUF conversion</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Converting model to GGUF...&#39;</span><span class="p">)</span>
<span class="n">llama_cpp_method</span> <span class="o">=</span> <span class="s2">&quot;q4_K_M&quot;</span>
<span class="n">convert_cmd_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">llama_cpp_path</span><span class="p">,</span> <span class="s2">&quot;convert.py&quot;</span><span class="p">)</span>
<span class="n">quantize_cmd_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">llama_cpp_path</span><span class="p">,</span> <span class="s2">&quot;quantize&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">llama_cpp_path</span><span class="p">):</span>
    <span class="n">cmd</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;git clone https://github.com/ggerganov/llama.cpp.git </span><span class="si">{</span><span class="n">llama_cpp_path</span><span class="si">}</span><span class="s2"> &amp;&amp; cd </span><span class="si">{</span><span class="n">llama_cpp_path</span><span class="si">}</span><span class="s2"> &amp;&amp; make LLAMA_CUBLAS=1 LLAMA_CUDA_F16=1&quot;</span>
    <span class="n">subprocess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">cmd</span><span class="p">],</span> <span class="n">shell</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">check</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">subprocess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span>
    <span class="sa">f</span><span class="s2">&quot;python </span><span class="si">{</span><span class="n">convert_cmd_path</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">quant_path</span><span class="si">}</span><span class="s2"> --outfile </span><span class="si">{</span><span class="n">quant_path</span><span class="si">}</span><span class="s2">/model.gguf&quot;</span>
<span class="p">],</span> <span class="n">shell</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">check</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">subprocess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">quantize_cmd_path</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">quant_path</span><span class="si">}</span><span class="s2">/model.gguf </span><span class="si">{</span><span class="n">quant_path</span><span class="si">}</span><span class="s2">/model_</span><span class="si">{</span><span class="n">llama_cpp_method</span><span class="si">}</span><span class="s2">.gguf </span><span class="si">{</span><span class="n">llama_cpp_method</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="p">],</span> <span class="n">shell</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">check</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<h3 id="custom-quantizer-qwen2-vl-example">Custom Quantizer (Qwen2 VL Example)<a class="headerlink" href="#custom-quantizer-qwen2-vl-example" title="Permanent link">&para;</a></h3>
<p>Below, the Qwen team has provided an example of how to use a custom quantizer. This works to
effectively quantize the Qwen2 VL model using multimodal examples.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">awq</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoAWQForCausalLM</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">awq.utils.qwen_vl_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">process_vision_info</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">awq.quantize.quantizer</span><span class="w"> </span><span class="kn">import</span> <span class="n">AwqQuantizer</span><span class="p">,</span> <span class="n">clear_memory</span><span class="p">,</span> <span class="n">get_best_device</span>

<span class="c1"># Specify paths and hyperparameters for quantization</span>
<span class="n">model_path</span> <span class="o">=</span> <span class="s2">&quot;Qwen/Qwen2-VL-7B-Instruct&quot;</span>
<span class="n">quant_path</span> <span class="o">=</span> <span class="s2">&quot;qwen2-vl-7b-instruct&quot;</span>
<span class="n">quant_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;zero_point&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="s2">&quot;q_group_size&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span> <span class="s2">&quot;w_bit&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;version&quot;</span><span class="p">:</span> <span class="s2">&quot;GEMM&quot;</span><span class="p">}</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoAWQForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">model_path</span><span class="p">,</span> <span class="n">attn_implementation</span><span class="o">=</span><span class="s2">&quot;flash_attention_2&quot;</span>
<span class="p">)</span>

<span class="c1"># We define our own quantizer by extending the AwqQuantizer.</span>
<span class="c1"># The main difference is in how the samples are processed when</span>
<span class="c1"># the quantization process initialized.</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Qwen2VLAwqQuantizer</span><span class="p">(</span><span class="n">AwqQuantizer</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">init_quant</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">modules</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">awq_model</span><span class="o">.</span><span class="n">get_model_layers</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">calib_data</span>

        <span class="n">inps</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">layer_kwargs</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="n">best_device</span> <span class="o">=</span> <span class="n">get_best_device</span><span class="p">()</span>
        <span class="n">modules</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">modules</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">best_device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">awq_model</span><span class="o">.</span><span class="n">move_embed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">best_device</span><span class="p">)</span>

        <span class="c1"># get input and kwargs to layer 0</span>
        <span class="c1"># with_kwargs is only supported in PyTorch 2.0</span>
        <span class="c1"># use this Catcher hack for now</span>
        <span class="k">class</span><span class="w"> </span><span class="nc">Catcher</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
            <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">):</span>
                <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">module</span> <span class="o">=</span> <span class="n">module</span>

            <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
                <span class="c1"># assume first input to forward is hidden states</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                    <span class="k">del</span> <span class="n">args</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">first_key</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span>
                    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">first_key</span><span class="p">)</span>

                <span class="n">inps</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
                <span class="n">layer_kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>
                <span class="k">raise</span> <span class="ne">ValueError</span>  <span class="c1"># early exit to break later inference</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">move_to_device</span><span class="p">(</span><span class="n">obj</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
            <span class="k">def</span><span class="w"> </span><span class="nf">get_device</span><span class="p">(</span><span class="n">obj</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                    <span class="k">return</span> <span class="n">obj</span><span class="o">.</span><span class="n">device</span>
                <span class="k">return</span> <span class="nb">next</span><span class="p">(</span><span class="n">obj</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span>

            <span class="k">if</span> <span class="n">get_device</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span> <span class="o">!=</span> <span class="n">device</span><span class="p">:</span>
                <span class="n">obj</span> <span class="o">=</span> <span class="n">obj</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">obj</span>

        <span class="c1"># patch layer 0 to catch input and kwargs</span>
        <span class="n">modules</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">Catcher</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">samples</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)):</span>
                <span class="n">samples</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">move_to_device</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">best_device</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">samples</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>  <span class="c1"># work with early exit</span>
            <span class="k">pass</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">samples</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)):</span>
                    <span class="n">samples</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">move_to_device</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
        <span class="n">modules</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">modules</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">module</span>  <span class="c1"># restore</span>

        <span class="k">del</span> <span class="n">samples</span>
        <span class="n">inps</span> <span class="o">=</span> <span class="n">inps</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">modules</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">modules</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">awq_model</span><span class="o">.</span><span class="n">move_embed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

        <span class="n">clear_memory</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">modules</span><span class="p">,</span> <span class="n">layer_kwargs</span><span class="p">,</span> <span class="n">inps</span>

<span class="c1"># Then you need to prepare your data for calibaration. What you need to do is just put samples into a list,</span>
<span class="c1"># each of which is a typical chat message as shown below. you can specify text and image in `content` field:</span>
<span class="c1"># dataset = [</span>
<span class="c1">#     # message 0</span>
<span class="c1">#     [</span>
<span class="c1">#         {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},</span>
<span class="c1">#         {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Tell me who you are.&quot;},</span>
<span class="c1">#         {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;I am a large language model named Qwen...&quot;},</span>
<span class="c1">#     ],</span>
<span class="c1">#     # message 1</span>
<span class="c1">#     [</span>
<span class="c1">#         {</span>
<span class="c1">#             &quot;role&quot;: &quot;user&quot;,</span>
<span class="c1">#             &quot;content&quot;: [</span>
<span class="c1">#                 {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;file:///path/to/your/image.jpg&quot;},</span>
<span class="c1">#                 {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Output all text in the image&quot;},</span>
<span class="c1">#             ],</span>
<span class="c1">#         },</span>
<span class="c1">#         {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;The text in the image is balabala...&quot;},</span>
<span class="c1">#     ],</span>
<span class="c1">#     # other messages...</span>
<span class="c1">#     ...,</span>
<span class="c1"># ]</span>
<span class="c1"># here, we use a caption dataset **only for demonstration**. You should replace it with your own sft dataset.</span>
<span class="k">def</span><span class="w"> </span><span class="nf">prepare_dataset</span><span class="p">(</span><span class="n">n_sample</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">]]:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>

    <span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;laion/220k-GPT4Vision-captions-from-LIVIS&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;train[:</span><span class="si">{</span><span class="n">n_sample</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span>
        <span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
                <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="p">[</span>
                    <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;image&quot;</span><span class="p">,</span> <span class="s2">&quot;image&quot;</span><span class="p">:</span> <span class="n">sample</span><span class="p">[</span><span class="s2">&quot;url&quot;</span><span class="p">]},</span>
                    <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;generate a caption for this image&quot;</span><span class="p">},</span>
                <span class="p">],</span>
            <span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">sample</span><span class="p">[</span><span class="s2">&quot;caption&quot;</span><span class="p">]},</span>
        <span class="p">]</span>
        <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">dataset</span>
    <span class="p">]</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">prepare_dataset</span><span class="p">()</span>

<span class="c1"># process the dataset into tensors</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">processor</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">image_inputs</span><span class="p">,</span> <span class="n">video_inputs</span> <span class="o">=</span> <span class="n">process_vision_info</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">processor</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> <span class="n">images</span><span class="o">=</span><span class="n">image_inputs</span><span class="p">,</span> <span class="n">videos</span><span class="o">=</span><span class="n">video_inputs</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>

<span class="c1"># Then just run the calibration process by one line of code</span>
<span class="n">model</span><span class="o">.</span><span class="n">quantize</span><span class="p">(</span><span class="n">calib_data</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">quant_config</span><span class="o">=</span><span class="n">quant_config</span><span class="p">,</span> <span class="n">quantizer_cls</span><span class="o">=</span><span class="n">Qwen2VLAwqQuantizer</span><span class="p">)</span>

<span class="c1"># Save the model</span>
<span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">generation_config</span><span class="o">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">model</span><span class="o">.</span><span class="n">save_quantized</span><span class="p">(</span><span class="n">quant_path</span><span class="p">,</span> <span class="n">safetensors</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">shard_size</span><span class="o">=</span><span class="s2">&quot;4GB&quot;</span><span class="p">)</span>
</code></pre></div>
<h3 id="another-custom-quantizer-minicpm3-example">Another Custom Quantizer (MiniCPM3 Example)<a class="headerlink" href="#another-custom-quantizer-minicpm3-example" title="Permanent link">&para;</a></h3>
<p>Here we introduce another custom quantizer from the MiniCPM team at OpenBMB. We only
modify the weight clipping mechanism to make quantization work.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">awq</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoAWQForCausalLM</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">awq.quantize.quantizer</span><span class="w"> </span><span class="kn">import</span> <span class="n">AwqQuantizer</span><span class="p">,</span> <span class="n">clear_memory</span>

<span class="k">class</span><span class="w"> </span><span class="nc">CPM3AwqQuantizer</span><span class="p">(</span><span class="n">AwqQuantizer</span><span class="p">):</span>
    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_compute_best_clip</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">w</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">input_feat</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">n_grid</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
        <span class="n">max_shrink</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="n">n_sample_token</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">assert</span> <span class="n">w</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span>
        <span class="n">org_w_shape</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span>
        <span class="c1"># w           [co, ci]      -&gt; [co, 1, n_group, group size]</span>
        <span class="c1"># input_feat  [n_token, ci] -&gt; [1, n_token, n_group, group size]</span>
        <span class="n">group_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">group_size</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">group_size</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">org_w_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">input_feat</span> <span class="o">=</span> <span class="n">input_feat</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_feat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">input_feat</span> <span class="o">=</span> <span class="n">input_feat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_feat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">group_size</span><span class="p">)</span>

        <span class="c1"># Compute input feature step size (minimum 1)</span>
        <span class="n">step_size</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_feat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="n">n_sample_token</span><span class="p">)</span>
        <span class="n">input_feat</span> <span class="o">=</span> <span class="n">input_feat</span><span class="p">[:,</span> <span class="p">::</span><span class="n">step_size</span><span class="p">]</span>

        <span class="n">w</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">org_w_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">group_size</span><span class="p">)</span>

        <span class="n">oc_batch_size</span> <span class="o">=</span> <span class="mi">256</span> <span class="k">if</span> <span class="n">org_w_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">%</span> <span class="mi">256</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">64</span>  <span class="c1"># prevent OOM</span>
        <span class="k">if</span> <span class="n">org_w_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">%</span> <span class="n">oc_batch_size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">oc_batch_size</span> <span class="o">=</span> <span class="n">org_w_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">assert</span> <span class="n">org_w_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">%</span> <span class="n">oc_batch_size</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="n">w_all</span> <span class="o">=</span> <span class="n">w</span>
        <span class="n">best_max_val_all</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">i_b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">org_w_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">oc_batch_size</span><span class="p">):</span>
            <span class="n">w</span> <span class="o">=</span> <span class="n">w_all</span><span class="p">[</span><span class="n">i_b</span> <span class="o">*</span> <span class="n">oc_batch_size</span> <span class="p">:</span> <span class="p">(</span><span class="n">i_b</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">oc_batch_size</span><span class="p">]</span>

            <span class="n">org_max_val</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># co, 1, n_group, 1</span>

            <span class="n">best_max_val</span> <span class="o">=</span> <span class="n">org_max_val</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            <span class="n">min_errs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">org_max_val</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1e9</span>
            <span class="n">input_feat</span> <span class="o">=</span> <span class="n">input_feat</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">org_out</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_feat</span> <span class="o">*</span> <span class="n">w</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># co, n_token, n_group</span>

            <span class="k">for</span> <span class="n">i_s</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">max_shrink</span> <span class="o">*</span> <span class="n">n_grid</span><span class="p">)):</span>
                <span class="n">max_val</span> <span class="o">=</span> <span class="n">org_max_val</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">i_s</span> <span class="o">/</span> <span class="n">n_grid</span><span class="p">)</span>
                <span class="n">min_val</span> <span class="o">=</span> <span class="o">-</span><span class="n">max_val</span>
                <span class="n">cur_w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">min_val</span><span class="p">,</span> <span class="n">max_val</span><span class="p">)</span>
                <span class="n">q_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pseudo_quantize_tensor</span><span class="p">(</span><span class="n">cur_w</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">cur_out</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_feat</span> <span class="o">*</span> <span class="n">q_w</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

                <span class="c1"># co, 1, n_group, 1</span>
                <span class="n">err</span> <span class="o">=</span> <span class="p">(</span><span class="n">cur_out</span> <span class="o">-</span> <span class="n">org_out</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">min_errs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
                <span class="k">del</span> <span class="n">cur_w</span>
                <span class="k">del</span> <span class="n">cur_out</span>
                <span class="n">cur_best_idx</span> <span class="o">=</span> <span class="n">err</span> <span class="o">&lt;</span> <span class="n">min_errs</span>
                <span class="n">min_errs</span><span class="p">[</span><span class="n">cur_best_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">err</span><span class="p">[</span><span class="n">cur_best_idx</span><span class="p">]</span>
                <span class="n">best_max_val</span><span class="p">[</span><span class="n">cur_best_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_val</span><span class="p">[</span><span class="n">cur_best_idx</span><span class="p">]</span>
            <span class="n">best_max_val_all</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">best_max_val</span><span class="p">)</span>

        <span class="n">best_max_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">best_max_val_all</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">clear_memory</span><span class="p">(</span><span class="n">input_feat</span><span class="p">)</span>
        <span class="n">clear_memory</span><span class="p">(</span><span class="n">org_out</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">best_max_val</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">model_path</span> <span class="o">=</span> <span class="s1">&#39;openbmb/MiniCPM3-4B&#39;</span>
<span class="n">quant_path</span> <span class="o">=</span> <span class="s1">&#39;minicpm3-4b-awq&#39;</span>
<span class="n">quant_config</span> <span class="o">=</span> <span class="p">{</span> <span class="s2">&quot;zero_point&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="s2">&quot;q_group_size&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span> <span class="s2">&quot;w_bit&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;version&quot;</span><span class="p">:</span> <span class="s2">&quot;GEMM&quot;</span> <span class="p">}</span>

<span class="c1"># Load model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoAWQForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">safetensors</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Quantize</span>
<span class="n">model</span><span class="o">.</span><span class="n">quantize</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">quant_config</span><span class="o">=</span><span class="n">quant_config</span><span class="p">,</span> <span class="n">quantizer_cls</span><span class="o">=</span><span class="n">CPM3AwqQuantizer</span><span class="p">)</span>

<span class="c1"># Save quantized model</span>
<span class="n">model</span><span class="o">.</span><span class="n">save_quantized</span><span class="p">(</span><span class="n">quant_path</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">quant_path</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Model is quantized and saved at &quot;</span><span class="si">{</span><span class="n">quant_path</span><span class="si">}</span><span class="s1">&quot;&#39;</span><span class="p">)</span>
</code></pre></div>
<h2 id="basic-inference">Basic Inference<a class="headerlink" href="#basic-inference" title="Permanent link">&para;</a></h2>
<h3 id="inference-with-gpu">Inference With GPU<a class="headerlink" href="#inference-with-gpu" title="Permanent link">&para;</a></h3>
<p>To run inference, you often want to run with <code>fuse_layers=True</code> to get the claimed speedup in AutoAWQ.
Additionally, consider setting <code>max_seq_len</code> (default: 2048) as this will be the maximum context that the model can hold.</p>
<p>Notes:</p>
<ul>
<li>You can specify <code>use_exllama_v2=True</code> to enable ExLlamaV2 kernels during inference.</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">awq</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoAWQForCausalLM</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">TextStreamer</span>

<span class="n">quant_path</span> <span class="o">=</span> <span class="s2">&quot;TheBloke/Mistral-7B-Instruct-v0.2-AWQ&quot;</span>

<span class="c1"># Load model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoAWQForCausalLM</span><span class="o">.</span><span class="n">from_quantized</span><span class="p">(</span><span class="n">quant_path</span><span class="p">,</span> <span class="n">fuse_layers</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">quant_path</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">streamer</span> <span class="o">=</span> <span class="n">TextStreamer</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">skip_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Convert prompt to tokens</span>
<span class="n">prompt_template</span> <span class="o">=</span> <span class="s2">&quot;[INST] </span><span class="si">{prompt}</span><span class="s2"> [/INST]&quot;</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;You&#39;re standing on the surface of the Earth. &quot;</span>\
        <span class="s2">&quot;You walk one mile south, one mile west and one mile north. &quot;</span>\
        <span class="s2">&quot;You end up exactly where you started. Where are you?&quot;</span>

<span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
    <span class="n">prompt_template</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">),</span> 
    <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span>
<span class="p">)</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

<span class="c1"># Generate output</span>
<span class="n">generation_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">tokens</span><span class="p">,</span> 
    <span class="n">streamer</span><span class="o">=</span><span class="n">streamer</span><span class="p">,</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span>
<span class="p">)</span>
</code></pre></div>
<h3 id="inference-with-cpu">Inference With CPU<a class="headerlink" href="#inference-with-cpu" title="Permanent link">&para;</a></h3>
<p>To run inference with CPU , you should specify <code>use_ipex=True</code>. ipex is the backend for CPU including kernel for operators. ipex is intel_extension_for_pytorch package.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">awq</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoAWQForCausalLM</span>

<span class="n">quant_path</span> <span class="o">=</span> <span class="s2">&quot;TheBloke/Mistral-7B-Instruct-v0.2-AWQ&quot;</span>
<span class="c1"># Load model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoAWQForCausalLM</span><span class="o">.</span><span class="n">from_quantized</span><span class="p">(</span><span class="n">quant_path</span><span class="p">,</span> <span class="n">use_ipex</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<h3 id="transformers">Transformers<a class="headerlink" href="#transformers" title="Permanent link">&para;</a></h3>
<p>You can also load an AWQ model by using AutoModelForCausalLM, just make sure you have AutoAWQ installed.
Note that not all models will have fused modules when loading from transformers.
See more <a href="https://huggingface.co/docs/transformers/main/en/quantization/awq">documentation here</a>.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">TextStreamer</span>

<span class="c1"># NOTE: Must install from PR until merged</span>
<span class="c1"># pip install --upgrade git+https://github.com/younesbelkada/transformers.git@add-awq</span>
<span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;casperhansen/mistral-7b-instruct-v0.1-awq&quot;</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">model_id</span><span class="p">,</span> 
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> 
    <span class="n">low_cpu_mem_usage</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">streamer</span> <span class="o">=</span> <span class="n">TextStreamer</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">skip_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Convert prompt to tokens</span>
<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;[INST] What are the basic steps to use the Huggingface transformers library? [/INST]&quot;</span>

<span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
    <span class="n">text</span><span class="p">,</span> 
    <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span>
<span class="p">)</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

<span class="c1"># Generate output</span>
<span class="n">generation_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">tokens</span><span class="p">,</span> 
    <span class="n">streamer</span><span class="o">=</span><span class="n">streamer</span><span class="p">,</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span>
<span class="p">)</span>
</code></pre></div>
<h3 id="vllm">vLLM<a class="headerlink" href="#vllm" title="Permanent link">&para;</a></h3>
<p>You can also load AWQ models in <a href="https://github.com/vllm-project/vllm">vLLM</a>.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">asyncio</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">PreTrainedTokenizer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">vllm</span><span class="w"> </span><span class="kn">import</span> <span class="n">AsyncLLMEngine</span><span class="p">,</span> <span class="n">SamplingParams</span><span class="p">,</span> <span class="n">AsyncEngineArgs</span>

<span class="n">model_path</span> <span class="o">=</span> <span class="s2">&quot;casperhansen/mixtral-instruct-awq&quot;</span>

<span class="c1"># prompting</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;You&#39;re standing on the surface of the Earth. &quot;</span>\
         <span class="s2">&quot;You walk one mile south, one mile west and one mile north. &quot;</span>\
         <span class="s2">&quot;You end up exactly where you started. Where are you?&quot;</span><span class="p">,</span>

<span class="n">prompt_template</span> <span class="o">=</span> <span class="s2">&quot;[INST] </span><span class="si">{prompt}</span><span class="s2"> [/INST]&quot;</span>

<span class="c1"># sampling params</span>
<span class="n">sampling_params</span> <span class="o">=</span> <span class="n">SamplingParams</span><span class="p">(</span>
    <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.1</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span>
<span class="p">)</span>

<span class="c1"># tokenizer</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>

<span class="c1"># async engine args for streaming</span>
<span class="n">engine_args</span> <span class="o">=</span> <span class="n">AsyncEngineArgs</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model_path</span><span class="p">,</span>
    <span class="n">quantization</span><span class="o">=</span><span class="s2">&quot;awq&quot;</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float16&quot;</span><span class="p">,</span>
    <span class="n">max_model_len</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">enforce_eager</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">disable_log_requests</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">disable_log_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">AsyncLLMEngine</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">:</span> <span class="n">PreTrainedTokenizer</span><span class="p">):</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt_template</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">))</span><span class="o">.</span><span class="n">input_ids</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
        <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
        <span class="n">sampling_params</span><span class="o">=</span><span class="n">sampling_params</span><span class="p">,</span>
        <span class="n">request_id</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">prompt_token_ids</span><span class="o">=</span><span class="n">tokens</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">** Starting generation!</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">last_index</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">async</span> <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">[</span><span class="n">last_index</span><span class="p">:],</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">last_index</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">** Finished generation!</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">AsyncLLMEngine</span><span class="o">.</span><span class="n">from_engine_args</span><span class="p">(</span><span class="n">engine_args</span><span class="p">)</span>
    <span class="n">asyncio</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">generate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">))</span>
</code></pre></div>
<h3 id="llava-multimodal">LLaVa (multimodal)<a class="headerlink" href="#llava-multimodal" title="Permanent link">&para;</a></h3>
<p>AutoAWQ also supports the LLaVa model. You simply need to load an 
AutoProcessor to process the prompt and image to generate inputs for the AWQ model.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">PIL</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">awq</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoAWQForCausalLM</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoProcessor</span><span class="p">,</span> <span class="n">TextStreamer</span>

<span class="c1"># Load model</span>
<span class="n">quant_path</span> <span class="o">=</span> <span class="s2">&quot;casperhansen/llama3-llava-next-8b-awq&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoAWQForCausalLM</span><span class="o">.</span><span class="n">from_quantized</span><span class="p">(</span><span class="n">quant_path</span><span class="p">)</span>
<span class="n">processor</span> <span class="o">=</span> <span class="n">AutoProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">quant_path</span><span class="p">)</span>
<span class="n">streamer</span> <span class="o">=</span> <span class="n">TextStreamer</span><span class="p">(</span><span class="n">processor</span><span class="p">,</span> <span class="n">skip_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Define prompt</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">&lt;|im_start|&gt;system</span><span class="se">\n</span><span class="s2">Answer the questions.&lt;|im_end|&gt;</span>
<span class="s2">&lt;|im_start|&gt;user</span><span class="se">\n</span><span class="s2">&lt;image&gt;</span><span class="se">\n</span><span class="s2">What is shown in this image?&lt;|im_end|&gt;</span>
<span class="s2">&lt;|im_start|&gt;assistant</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="c1"># Define image</span>
<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true&quot;</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">raw</span><span class="p">)</span>

<span class="c1"># Load inputs</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>

<span class="n">generation_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="o">**</span><span class="n">inputs</span><span class="p">,</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">streamer</span><span class="o">=</span><span class="n">streamer</span>
<span class="p">)</span>
</code></pre></div>
<h3 id="qwen2-vl">Qwen2 VL<a class="headerlink" href="#qwen2-vl" title="Permanent link">&para;</a></h3>
<p>Below is an example of how to run inference using Qwen2 VL.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">awq</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoAWQForCausalLM</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">awq.utils.qwen_vl_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">process_vision_info</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoProcessor</span><span class="p">,</span> <span class="n">TextStreamer</span>

<span class="c1"># Load model</span>
<span class="n">quant_path</span> <span class="o">=</span> <span class="s2">&quot;Qwen/Qwen2-VL-7B-Instruct-AWQ&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoAWQForCausalLM</span><span class="o">.</span><span class="n">from_quantized</span><span class="p">(</span><span class="n">quant_path</span><span class="p">)</span>
<span class="n">processor</span> <span class="o">=</span> <span class="n">AutoProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">quant_path</span><span class="p">)</span>
<span class="n">streamer</span> <span class="o">=</span> <span class="n">TextStreamer</span><span class="p">(</span><span class="n">processor</span><span class="p">,</span> <span class="n">skip_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
        <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;image&quot;</span><span class="p">,</span>
                <span class="s2">&quot;image&quot;</span><span class="p">:</span> <span class="s2">&quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg&quot;</span><span class="p">,</span>
            <span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Describe this image.&quot;</span><span class="p">},</span>
        <span class="p">],</span>
    <span class="p">}</span>
<span class="p">]</span>

<span class="c1"># Load inputs</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">processor</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span>
    <span class="n">messages</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="n">image_inputs</span><span class="p">,</span> <span class="n">video_inputs</span> <span class="o">=</span> <span class="n">process_vision_info</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span>
    <span class="n">text</span><span class="o">=</span><span class="p">[</span><span class="n">text</span><span class="p">],</span>
    <span class="n">images</span><span class="o">=</span><span class="n">image_inputs</span><span class="p">,</span>
    <span class="n">videos</span><span class="o">=</span><span class="n">video_inputs</span><span class="p">,</span>
    <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>

<span class="n">generation_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="o">**</span><span class="n">inputs</span><span class="p">,</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">streamer</span><span class="o">=</span><span class="n">streamer</span>
<span class="p">)</span>
</code></pre></div>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": ["search.suggest", "search.highlight", "content.tabs.link", "navigation.indexes", "content.tooltips", "navigation.path", "content.code.annotate", "content.code.copy", "content.code.select", "navigation.tabs"], "search": "../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.13a4f30d.min.js"></script>
      
    
  </body>
</html>